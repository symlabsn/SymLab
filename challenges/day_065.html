<!doctype html>
<html lang="fr">
<head>
<meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>Jour 065 ‚Äî PCA : R√©duction Dim.</title>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
<style>
:root{--bg:#0f172a; --accent:#06b6d4; --muted:#94a3b8;}
*{box-sizing:border-box}body{margin:0;font-family:Inter,system-ui;background:linear-gradient(180deg,#f3f7fb,#eef7fb);color:#0b1220}
.header{background:linear-gradient(90deg,#0b3d91,#0ea5e9);color:white;padding:20px;border-radius:10px;margin:16px}
.container{max-width:1100px;margin:0 auto;padding:16px}
.page{background:white;padding:24px;border-radius:12px;box-shadow:0 12px 40px rgba(2,6,23,0.06);max-width:900px;margin:0 auto}
.page h1{margin:0 0 12px 0;font-size:2em}
.page h2{margin:24px 0 12px 0;color:#0b3d91;border-bottom:2px solid #e6f6fb;padding-bottom:8px}
.page h3{margin:20px 0 10px 0;color:#0ea5e9}
.meta{color:var(--muted);font-size:13px;margin-bottom:16px}
.analogy{background:linear-gradient(90deg,#fff7ed,#fff3e8);border-left:4px solid #f59e0b;padding:16px;border-radius:8px;margin:20px 0;font-style:italic}
.theory{background:#f8fafc;padding:16px;border-radius:8px;margin:20px 0;border:1px solid #e6eef8}
.scientist{background:white;border:1px solid #e6eef8;padding:12px;border-radius:8px;margin:10px 0}
.scientist-name{font-weight:700;color:#0b3d91}
.scientist-year{color:var(--muted);font-size:0.9em}
.code{background:#0b1220;color:#d1fae5;padding:16px;border-radius:8px;overflow:auto;font-family:monospace;margin:16px 0}
.output{background:linear-gradient(90deg,#fff7ed,#fff3e8);border-left:4px solid #f59e0b;padding:12px;border-radius:8px;margin-top:12px;color:#92400e}
.exercises{margin-top:20px;padding:16px;background:#f8fafc;border-radius:8px;border:1px solid #eef2ff}
.badge{display:inline-block;padding:4px 12px;border-radius:20px;font-size:0.85em;font-weight:600;margin:4px}
.badge-level{background:#e0f2fe;color:#0369a1}
.badge-mastery{background:#f3e8ff;color:#7c3aed}
.badge-xp{background:#fef3c7;color:#92400e}
.controls{display:flex;gap:12px;justify-content:space-between;margin-top:24px;padding-top:24px;border-top:2px solid #e6eef8}
.btn{background:var(--accent);color:white;padding:10px 16px;border-radius:8px;text-decoration:none;font-weight:600;display:inline-block}
</style>
</head>
<body>
<header class="header container">
  <div style="display:flex;justify-content:space-between;align-items:center">
    <div>
      <h1 style="margin:0;color:white">Jour 065 ‚Äî PCA : R√©duction Dim.</h1>
      <div style="font-size:0.9em;opacity:0.9">Challenge 100 Jours SymPy</div>
    </div>
    <div><a class="btn" href="index.html">‚Üê Index</a></div>
  </div>
</header>

<div class="container">
  <main class="page">
    <div class="meta">
      <span class="badge badge-level">üéØ Avanc√©</span>
      <span class="badge badge-mastery">üìä Expert</span>
      <span class="badge badge-xp">‚≠ê +45 XP</span>
    </div>

    <h2>üåç Analogie Africaine</h2>
    <div class="analogy">La PCA (Analyse en Composantes Principales) est comme prendre une photo d'une sculpture sous son meilleur angle. On passe de la 3D √† la 2D en gardant le plus d'informations possible (l'ombre la plus large). On simplifie la r√©alit√© sans la trahir.</div>

    <h2>üìö Th√©orie : Alg√®bre Lin√©aire Appliqu√©e</h2>
    <div class="theory">
      <p>Projette les donn√©es sur les axes de plus grande variance. Utilise la d√©composition en valeurs propres de la matrice de covariance.</p>
      <h3>Fondements Math√©matiques</h3>
      <ul><li>Matrice de covariance : Œ£ = (1/m) X·µÄX</li><li>Valeurs propres (Œª) et Vecteurs propres (v)</li><li>Variance expliqu√©e : Œª_i / Œ£Œª_j</li><li>Projection orthogonale</li><li>D√©corr√©lation des variables</li></ul>
    </div>

    <h2>üë®‚Äçüî¨ Histoire & Scientifiques</h2>
    
    <div class="scientist">
      <div class="scientist-name">Karl Pearson</div>
      <div class="scientist-year">1901</div>
      <div class="scientist-contribution">
        <strong>Contribution :</strong> Invention de la PCA
        <br><em>Contexte :</em> P√®re des statistiques modernes
      </div>
    </div>
    <div class="scientist">
      <div class="scientist-name">Harold Hotelling</div>
      <div class="scientist-year">1933</div>
      <div class="scientist-contribution">
        <strong>Contribution :</strong> D√©veloppement th√©orique
        <br><em>Contexte :</em> Popularise la m√©thode en √©conom√©trie
      </div>
    </div>

    <h2>üíª Code SymPy</h2>
    <div class="code"><pre>from sympy import Matrix, eye
# Matrice de covariance exemple (2D)
C = Matrix([[4, 2], [2, 3]])
# Diagonalisation pour trouver les axes principaux
P, D = C.diagonalize()
print(f'Valeurs propres (Variance) : {D}')</pre></div>
    <div class="output"><strong>Sortie attendue :</strong> Matrix([[2, 0], [0, 5]])</div>

    <h2>üí™ Exercices Pratiques</h2>
    <div class="exercises"><ol><li>Calculer les vecteurs propres d'une matrice 2x2</li><li>Pourcentage de variance expliqu√©e</li><li>Lien avec la SVD (Singular Value Decomposition)</li></ol></div>

    <h2>üî¨ Applications Pratiques</h2>
    <p>Compression de donn√©es, reconnaissance de visages (Eigenfaces), visualisation de donn√©es complexes.</p>

    <div class="controls">
      <a class="btn" href="day_064.html">‚Üê Pr√©c√©dent</a>
      <a class="btn" href="day_066.html">Suivant ‚Üí</a>
    </div>
  </main>
</div>
</body>
</html>