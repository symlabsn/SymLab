
export const machineLearningData = {
    id: 'ml-intro',
    title: 'Introduction au Machine Learning',
    chapters: [
        {
            id: 'ml-01',
            part: 'Partie 1 : Fondamentaux',
            title: '1. Qu\'est-ce que le Machine Learning ?',
            image: "https://images.unsplash.com/photo-1527474305487-b87b222841cc?q=80&w=2574&auto=format&fit=crop",
            story: "Imaginez un enfant apprenant √† reconna√Ætre un chat. On ne lui donne pas une d√©finition math√©matique ('oreilles pointues + moustaches'). On lui montre des images : '√áa c'est un chat', '√áa c'est un chien'. Le Machine Learning, c'est exactement √ßa : l'ordinateur apprend par l'exemple.",
            content: `
### 1. D√©finition Simplifi√©e
Le Machine Learning (Apprentissage Automatique) consiste √† cr√©er des programmes qui **s'am√©liorent avec l'exp√©rience** (les donn√©es) sans √™tre explicitement reprogramm√©s.

$$ \\text{Programme Classique} : \\text{Donn√©es} + \\text{R√®gles} = \\text{R√©ponses} $$
$$ \\text{Machine Learning} : \\text{Donn√©es} + \\text{R√©ponses} = \\text{R√®gles} $$

### 2. Les 3 Grands Types
- **Supervis√©** : On a les r√©ponses (labels).
    - *Exemple* : Pr√©dire le prix d'une maison (R√©gression) ou dire si un email est un spam (Classification).
- **Non-Supervis√©** : On n'a pas les r√©ponses, on cherche des structures.
    - *Exemple* : Grouper des clients similaires (Clustering).
- **Par Renforcement** : Apprendre par essai-erreur via des r√©compenses.
    - *Exemple* : Apprendre √† marcher √† un robot ou √† jouer √† Mario.

### 3. Le Workflow du Data Scientist
1.  **R√©colte** : Trouver des donn√©es propres.
2.  **Nettoyage (Preprocessing)** : G√©rer les valeurs manquantes, normaliser.
3.  **Entra√Ænement** : Le mod√®le apprend sur le *Training Set*.
4.  **√âvaluation** : On teste la performance sur le *Test Set*.
5.  **D√©ploiement** : Mise en production.
            `,
            summary: [
                "Le ML trouve des patterns que l'humain ne peut pas voir.",
                "La qualit√© des donn√©es (Data Quality) est plus critique que le choix de l'algorithme.",
                "Toujours s√©parer ses donn√©es en Train (80%) et Test (20%) pour v√©rifier que le mod√®le ne triche pas (Overfitting)."
            ],
            exercises: [
                {
                    id: 'quiz-ml-1',
                    question: "Si je veux grouper des articles de news par th√©matique sans avoir de cat√©gories pr√©d√©finies, j'utilise :",
                    options: ["Apprentissage Supervis√©", "Apprentissage Non-Supervis√©", "R√©gression Lin√©aire"],
                    correctAnswer: 1,
                    explanation: "C'est du clustering (Non-Supervis√©) car on n'a pas de labels (cat√©gories) au d√©part."
                },
                {
                    id: 'quiz-ml-workflow',
                    question: "Quelle √©tape prend g√©n√©ralement 80% du temps d'un Data Scientist ?",
                    options: ["Choisir l'algorithme", "Optimiser les param√®tres", "Nettoyer et pr√©parer les donn√©es"],
                    correctAnswer: 2,
                    explanation: "Le 'Data Cleaning' est la partie la plus longue et la plus ingrate mais essentielle."
                }
            ]
        },
        {
            id: 'ml-02',
            part: 'Partie 1 : Fondamentaux',
            title: '2. R√©gression Lin√©aire',
            image: "https://images.unsplash.com/photo-1551288049-bebda4e38f71?q=80&w=2670&auto=format&fit=crop",
            story: "En 1886, Francis Galton √©tudiait la taille des enfants par rapport √† celle de leurs parents. Il a d√©couvert un ph√©nom√®ne fascinant : les enfants de parents tr√®s grands tendent √† √™tre plus petits que leurs parents (r√©gression vers la moyenne). Pour mod√©liser cela, il a invent√© la r√©gression lin√©aire, l'algorithme le plus fondamental du Machine Learning.",
            content: `
### üéØ L'Objectif : Trouver la Meilleure Droite

Imaginez que vous avez des points sur un graphique (par exemple, surface d'une maison vs prix). Vous voulez tracer une droite qui passe **le plus pr√®s possible** de tous ces points.

**√âquation math√©matique** :
$$ y = wx + b $$

O√π :
- $x$ : La variable d'entr√©e (feature) - ex: surface en m¬≤
- $y$ : La variable √† pr√©dire (target) - ex: prix en ‚Ç¨
- $w$ : Le **poids** (weight) ou pente - mesure l'impact de x sur y
- $b$ : Le **biais** (bias) ou ordonn√©e √† l'origine - valeur de base

**Exemple concret** : Si $w = 3000$ et $b = 50000$, alors une maison de 100m¬≤ co√ªterait :
$$ y = 3000 \\times 100 + 50000 = 350\\,000‚Ç¨ $$

### üìê La Fonction de Co√ªt (Loss Function)

Comment savoir si notre droite est bonne ? On mesure l'**erreur** !

**MSE (Mean Squared Error)** :
$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$

O√π :
- $y_i$ : La vraie valeur (prix r√©el de la maison i)
- $\\hat{y}_i$ : La pr√©diction de notre mod√®le (prix pr√©dit)
- $n$ : Nombre d'exemples

**Pourquoi √©lever au carr√© ?**
1. Les erreurs n√©gatives et positives ne s'annulent pas
2. On p√©nalise plus fortement les grosses erreurs
3. C'est math√©matiquement d√©rivable (important pour l'optimisation)

### ‚õ∞Ô∏è Descente de Gradient : L'Algorithme Magique

**Analogie** : Vous √™tes en haut d'une montagne dans le brouillard. Vous voulez descendre au point le plus bas (le minimum de l'erreur). Comment faire ?

1. **T√¢ter le sol** : Calculer la pente (le gradient) autour de vous
2. **Faire un pas** : Avancer dans la direction qui descend le plus
3. **R√©p√©ter** : Jusqu'√† ce que vous ne puissiez plus descendre

**Formule math√©matique** :
$$ w_{nouveau} = w_{ancien} - \\alpha \\frac{\\partial MSE}{\\partial w} $$
$$ b_{nouveau} = b_{ancien} - \\alpha \\frac{\\partial MSE}{\\partial b} $$

O√π $\\alpha$ est le **learning rate** (taux d'apprentissage).

### üéõÔ∏è Le Learning Rate : Un Param√®tre Critique

- **Trop grand** ($\\alpha = 0.1$) : Vous sautez d'un c√¥t√© √† l'autre de la vall√©e, vous divergez
- **Trop petit** ($\\alpha = 0.0001$) : Vous avancez √† pas de fourmi, √ßa prend des heures
- **Optimal** ($\\alpha = 0.01$) : Vous convergez rapidement vers le minimum

### üíª Impl√©mentation en Python (from scratch)

\`\`\`python
import numpy as np
import matplotlib.pyplot as plt

# Donn√©es d'exemple : Surface (m¬≤) vs Prix (k‚Ç¨)
X = np.array([50, 60, 70, 80, 90, 100, 110, 120])
y = np.array([150, 180, 200, 220, 250, 270, 290, 320])

# Initialisation al√©atoire
w = 0.0
b = 0.0
learning_rate = 0.01
epochs = 1000

# Descente de gradient
for epoch in range(epochs):
    # Pr√©dictions
    y_pred = w * X + b
    
    # Calcul de l'erreur (MSE)
    mse = np.mean((y - y_pred) ** 2)
    
    # Calcul des gradients
    dw = -2 * np.mean(X * (y - y_pred))
    db = -2 * np.mean(y - y_pred)
    
    # Mise √† jour des param√®tres
    w = w - learning_rate * dw
    b = b - learning_rate * db
    
    if epoch % 100 == 0:
        print(f"Epoch {epoch}: MSE = {mse:.2f}, w = {w:.2f}, b = {b:.2f}")

print(f"\\nMod√®le final: y = {w:.2f}x + {b:.2f}")

# Pr√©diction pour une nouvelle maison de 95m¬≤
nouvelle_surface = 95
prix_predit = w * nouvelle_surface + b
print(f"Prix pr√©dit pour 95m¬≤: {prix_predit:.2f}k‚Ç¨")
\`\`\`

### üìä Avec Scikit-Learn (la vraie vie)

\`\`\`python
from sklearn.linear_model import LinearRegression
import numpy as np

# Donn√©es
X = np.array([[50], [60], [70], [80], [90], [100], [110], [120]])
y = np.array([150, 180, 200, 220, 250, 270, 290, 320])

# Cr√©er et entra√Æner le mod√®le
model = LinearRegression()
model.fit(X, y)

# Afficher les param√®tres
print(f"Poids (w): {model.coef_[0]:.2f}")
print(f"Biais (b): {model.intercept_:.2f}")

# Pr√©dire
prix = model.predict([[95]])
print(f"Prix pr√©dit pour 95m¬≤: {prix[0]:.2f}k‚Ç¨")

# Score R¬≤ (coefficient de d√©termination)
score = model.score(X, y)
print(f"R¬≤ score: {score:.3f}")  # 1.0 = parfait, 0.0 = inutile
\`\`\`

### üîç √âvaluation du Mod√®le

**R¬≤ (Coefficient de D√©termination)** :
$$ R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2} $$

- **R¬≤ = 1** : Le mod√®le explique 100% de la variance (parfait)
- **R¬≤ = 0.8** : Le mod√®le explique 80% de la variance (tr√®s bon)
- **R¬≤ = 0** : Le mod√®le n'est pas meilleur qu'une simple moyenne
- **R¬≤ < 0** : Le mod√®le est pire qu'une moyenne (catastrophe)

### ‚ö†Ô∏è Limites de la R√©gression Lin√©aire

1. **Hypoth√®se de lin√©arit√©** : Si la relation n'est pas lin√©aire (ex: exponentielle), √ßa ne marchera pas
2. **Sensible aux outliers** : Un seul point aberrant peut fausser toute la droite
3. **Multicollin√©arit√©** : Si deux variables sont tr√®s corr√©l√©es, le mod√®le devient instable
4. **Extrapolation dangereuse** : Pr√©dire en dehors de la plage des donn√©es d'entra√Ænement est risqu√©
            `,
            summary: [
                "La r√©gression lin√©aire cherche la droite qui minimise l'erreur quadratique moyenne (MSE).",
                "La descente de gradient est l'algorithme d'optimisation : on suit la pente pour descendre vers le minimum.",
                "Le learning rate est crucial : trop grand = divergence, trop petit = lenteur.",
                "R¬≤ mesure la qualit√© du mod√®le : 1 = parfait, 0 = inutile.",
                "Attention aux outliers et √† l'extrapolation en dehors des donn√©es d'entra√Ænement."
            ],
            exercises: [
                {
                    id: 'quiz-ml-2',
                    question: "Si ma *Loss* (Erreur) est proche de 0, cela signifie que :",
                    options: ["Mon mod√®le est mauvais", "Mon mod√®le pr√©dit presque parfaitement les donn√©es d'entra√Ænement", "Il y a un bug"],
                    correctAnswer: 1,
                    explanation: "Une erreur faible signifie que les pr√©dictions sont tr√®s proches des valeurs r√©elles. Attention cependant √† l'overfitting !"
                },
                {
                    id: 'quiz-ml-lr-grad',
                    question: "Que se passe-t-il si le learning rate est trop √©lev√© ?",
                    options: ["L'entra√Ænement est plus rapide", "Le mod√®le diverge et l'erreur explose", "Le mod√®le converge lentement"],
                    correctAnswer: 1,
                    explanation: "Un learning rate trop √©lev√© fait 'sauter' les param√®tres d'un c√¥t√© √† l'autre du minimum, emp√™chant la convergence."
                }
            ]
        },
        {
            id: 'ml-03',
            part: 'Partie 2 : Algorithmes Classiques',
            title: '3. Classification : Logistic Regression & KNN',
            image: "https://images.unsplash.com/photo-1599658880436-c61792e70672?q=80&w=2670&auto=format&fit=crop",
            story: "Comment votre banque d√©cide-t-elle si une transaction est frauduleuse ou non ? C'est binaire : Oui/Non. La r√©gression lin√©aire ne marche pas ici, il nous faut... la Logistique.",
            content: `
### R√©gression Logistique
Malgr√© son nom, c'est un algo de **Classification**.
Elle n'utilise pas une droite, mais une **Sigmo√Øde** (courbe en S) qui √©crase les valeurs entre 0 et 1.
$$ P(y=1|x) = \\frac{1}{1 + e^{-(wx+b)}} $$
Si probabilit√© > 0.5 $\\rightarrow$ Classe 1. Sinon Classe 0.

### K-Nearest Neighbors (KNN)
"Dis-moi qui sont tes voisins, je te dirai qui tu es."
Pour classer un nouveau point, on regarde les **K** points les plus proches dans les donn√©es d'entra√Ænement.
- Si 3 voisins sont rouges et 2 sont bleus $\\rightarrow$ Je suis Rouge.
- C'est un algorithme "paresseux" (Lazy Learning) car il n'apprend pas de mod√®le, il stocke juste les donn√©es.
            `,
            summary: [
                "Classification = Pr√©dire une cat√©gorie (Chat/Chien, Malade/Sain).",
                "La matrice de confusion permet d'√©valuer les erreurs (Faux Positifs vs Faux N√©gatifs).",
                "KNN est simple mais lent si on a beaucoup de donn√©es (car il doit calculer toutes les distances)."
            ],
            exercises: [
                {
                    id: 'quiz-ml-knn',
                    question: "Avec K=1 dans KNN, le mod√®le risque de...",
                    options: ["√ätre trop simple (Underfitting)", "√ätre trop sensible au bruit (Overfitting)", "√ätre parfait"],
                    correctAnswer: 1,
                    explanation: "Avec K=1, le mod√®le copie simplement le point le plus proche, m√™me si c'est une aberration (bruit). Il manque de g√©n√©ralisation."
                }
            ]
        },
        {
            id: 'ml-04',
            part: 'Partie 3 : R√©seaux de Neurones',
            title: '4. Introduction au Deep Learning',
            image: "https://images.unsplash.com/photo-1620712943543-bcc4688e7485?q=80&w=2565&auto=format&fit=crop",
            story: "Le cerveau humain contient 86 milliards de neurones. Le Deep Learning tente d'imiter cette structure en empilant des couches de neurones artificiels pour comprendre des donn√©es complexes comme des images ou du texte.",
            content: `
### Le Neurone Artificiel
C'est une fonction math√©matique simple :
1.  Il re√ßoit des entr√©es ($x_1, x_2...$).
2.  Il fait une somme pond√©r√©e ($w_1 x_1 + w_2 x_2...$).
3.  Il applique une **Fonction d'Activation** (ReLU, Sigmoid) pour d√©cider s'il "tire" ou non.

### R√©seaux de Neurones (Neural Networks)
On connecte ces neurones en couches :
- **Couche d'entr√©e** : Les pixels de l'image.
- **Couches cach√©es** (Hidden Layers) : D√©tectent des formes (bords, cercles, yeux...).
- **Couche de sortie** : La d√©cision finale ("C'est un Chat").

Le **Deep Learning**, c'est quand il y a beaucoup de couches cach√©es (+ de profondeur).

### Backpropagation (R√©tropropagation)
C'est l'algorithme cl√©. Quand le r√©seau se trompe, il calcule l'erreur et remonte en arri√®re pour corriger un tout petit peu les poids de chaque neurone. Apr√®s des millions d'exemples, il devient expert.
            `,
            summary: [
                "Le Deep Learning excelle sur les donn√©es non structur√©es (Images, Son, Texte).",
                "Il n√©cessite beaucoup de donn√©es et beaucoup de puissance de calcul (GPU).",
                "C'est une 'Black Box' : il est difficile d'expliquer pourquoi le r√©seau a pris cette d√©cision."
            ],
            exercises: [
                {
                    id: 'quiz-ml-dl',
                    question: "Quel composant permet √† un r√©seau de neurones d'apprendre des relations non-lin√©aires complexes ?",
                    options: ["Les poids", "Le biais", "La fonction d'activation"],
                    correctAnswer: 2,
                    explanation: "Sans fonction d'activation (comme ReLU), empiler des couches revient √† faire une simple multiplication g√©ante. La non-lin√©arit√© est essentielle."
                }
            ]
        },
        {
            id: 'ml-05',
            part: 'Partie 4 : Algorithmes Avanc√©s',
            title: '5. Arbres de D√©cision & Random Forest',
            image: "https://images.unsplash.com/photo-1542273917363-3b1817f69a2d?q=80&w=2574&auto=format&fit=crop",
            story: "Imaginez un m√©decin qui diagnostique une maladie. Il pose des questions : 'Avez-vous de la fi√®vre ?' Si oui, 'Depuis combien de jours ?'. C'est exactement comme √ßa que fonctionne un arbre de d√©cision : une s√©rie de questions binaires.",
            content: `
### Arbre de D√©cision
C'est un mod√®le qui ressemble √† un organigramme :
- Chaque **n≈ìud** pose une question sur une variable.
- Chaque **branche** est une r√©ponse possible.
- Chaque **feuille** est une pr√©diction finale.

**Avantages** :
- Tr√®s interpr√©table (on peut dessiner l'arbre).
- G√®re les donn√©es cat√©gorielles et num√©riques.
- Pas besoin de normaliser les donn√©es.

**Inconv√©nients** :
- Tendance √† l'overfitting (arbre trop profond).
- Instable (un petit changement dans les donn√©es peut changer tout l'arbre).

### Random Forest (For√™t Al√©atoire)
Pour corriger l'instabilit√©, on cr√©e **plusieurs arbres** (une for√™t) et on fait voter :
- Chaque arbre est entra√Æn√© sur un sous-ensemble al√©atoire des donn√©es.
- La pr√©diction finale est la moyenne (r√©gression) ou le vote majoritaire (classification).

C'est un exemple de **Ensemble Learning** : combiner plusieurs mod√®les faibles pour en faire un fort.

\`\`\`python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# Charger les donn√©es
iris = load_iris()
X, y = iris.data, iris.target

# Entra√Æner
rf = RandomForestClassifier(n_estimators=100, max_depth=3)
rf.fit(X, y)

# Pr√©dire
prediction = rf.predict([[5.1, 3.5, 1.4, 0.2]])
print(f"Esp√®ce pr√©dite : {iris.target_names[prediction[0]]}")
\`\`\`
            `,
            summary: [
                "Les arbres sont faciles √† comprendre mais instables.",
                "Random Forest am√©liore la robustesse en moyennant plusieurs arbres.",
                "Tr√®s performant en pratique, souvent utilis√© en comp√©tition Kaggle."
            ],
            exercises: [
                {
                    id: 'quiz-ml-tree',
                    question: "Pourquoi Random Forest est-il plus robuste qu'un seul arbre de d√©cision ?",
                    options: ["Il est plus rapide", "Il combine plusieurs arbres pour r√©duire la variance", "Il utilise moins de m√©moire"],
                    correctAnswer: 1,
                    explanation: "En moyennant les pr√©dictions de plusieurs arbres entra√Æn√©s sur des donn√©es diff√©rentes, Random Forest r√©duit l'overfitting et la variance."
                }
            ]
        },
        {
            id: 'ml-06',
            part: 'Partie 4 : Algorithmes Avanc√©s',
            title: '6. Clustering : K-Means & DBSCAN',
            image: "https://images.unsplash.com/photo-1551288049-bebda4e38f71?q=80&w=2670&auto=format&fit=crop",
            story: "Netflix ne sait pas √† l'avance combien de 'types' de spectateurs existent. Mais en analysant les habitudes de visionnage, il peut d√©couvrir des groupes : les fans d'action, les amateurs de com√©dies romantiques, etc. C'est le clustering.",
            content: `
### K-Means
L'algorithme le plus populaire pour grouper des donn√©es.

**Comment √ßa marche ?**
1.  Choisir K (nombre de clusters).
2.  Placer K centro√Ødes au hasard.
3.  Assigner chaque point au centro√Øde le plus proche.
4.  Recalculer les centro√Ødes (moyenne des points du cluster).
5.  R√©p√©ter jusqu'√† convergence.

**Exemple : Segmentation Client**
\`\`\`python
from sklearn.cluster import KMeans
import numpy as np

# Donn√©es : [√Çge, Revenu]
X = np.array([[25, 30000], [30, 40000], [35, 50000], 
              [45, 60000], [50, 70000], [55, 80000]])

# K-Means avec 2 clusters
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(X)

# Pr√©dire le cluster d'un nouveau client
nouveau_client = [[28, 35000]]
cluster = kmeans.predict(nouveau_client)
print(f"Ce client appartient au cluster {cluster[0]}")
\`\`\`

### DBSCAN (Density-Based)
Contrairement √† K-Means, DBSCAN ne n√©cessite pas de sp√©cifier K.
Il trouve des clusters de **densit√©** : les zones o√π les points sont proches.
- Avantage : D√©tecte les formes complexes et les outliers.
- Inconv√©nient : Sensible aux param√®tres (epsilon, min_samples).
            `,
            summary: [
                "K-Means est simple et rapide mais n√©cessite de choisir K √† l'avance.",
                "DBSCAN est plus flexible et d√©tecte les outliers automatiquement.",
                "Le clustering est non-supervis√© : on d√©couvre la structure cach√©e des donn√©es."
            ],
            exercises: [
                {
                    id: 'quiz-ml-cluster',
                    question: "Quelle est la principale limitation de K-Means ?",
                    options: ["Il est trop lent", "Il faut sp√©cifier le nombre de clusters K √† l'avance", "Il ne fonctionne que sur des images"],
                    correctAnswer: 1,
                    explanation: "K-Means n√©cessite de d√©finir K au d√©part, ce qui peut √™tre difficile si on ne conna√Æt pas la structure des donn√©es. Des m√©thodes comme la 'm√©thode du coude' aident √† choisir K."
                }
            ]
        },
        {
            id: 'ml-07',
            part: 'Partie 5 : Projet Pratique',
            title: '7. Projet : Pr√©diction de Prix Immobiliers',
            image: "https://images.unsplash.com/photo-1560518883-ce09059eeffa?q=80&w=2573&auto=format&fit=crop",
            story: "Vous √™tes Data Scientist dans une agence immobili√®re. Votre mission : cr√©er un mod√®le qui pr√©dit le prix d'une maison en fonction de ses caract√©ristiques (surface, nombre de chambres, quartier). C'est parti !",
            content: `
### √âtape 1 : Chargement et Exploration
\`\`\`python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# Charger les donn√©es (exemple fictif)
df = pd.read_csv('houses.csv')
# Colonnes : Surface, Chambres, Salles_Bain, Quartier, Prix

# Explorer
print(df.head())
print(df.describe())
print(df.isnull().sum())  # Valeurs manquantes ?
\`\`\`

### √âtape 2 : Preprocessing
\`\`\`python
# Encoder les variables cat√©gorielles (Quartier)
df = pd.get_dummies(df, columns=['Quartier'], drop_first=True)

# S√©parer X (features) et y (target)
X = df.drop('Prix', axis=1)
y = df['Prix']

# Split Train/Test (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
\`\`\`

### √âtape 3 : Entra√Ænement
\`\`\`python
# Mod√®le 1 : R√©gression Lin√©aire
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

# Mod√®le 2 : Random Forest
rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
\`\`\`

### √âtape 4 : √âvaluation
\`\`\`python
# M√©triques
mae_lr = mean_absolute_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)

mae_rf = mean_absolute_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f"R√©gression Lin√©aire - MAE: {mae_lr:.2f}, R¬≤: {r2_lr:.3f}")
print(f"Random Forest - MAE: {mae_rf:.2f}, R¬≤: {r2_rf:.3f}")
\`\`\`

### √âtape 5 : Interpr√©tation
- **MAE (Mean Absolute Error)** : Erreur moyenne en euros. Plus c'est bas, mieux c'est.
- **R¬≤ (Coefficient de D√©termination)** : % de variance expliqu√©e. R¬≤=1 = parfait, R¬≤=0 = inutile.
- **Feature Importance** : Quelles variables sont les plus importantes ?

\`\`\`python
# Importance des features (Random Forest)
importances = rf.feature_importances_
features = X.columns
for f, imp in sorted(zip(features, importances), key=lambda x: x[1], reverse=True):
    print(f"{f}: {imp:.3f}")
\`\`\`
            `,
            summary: [
                "Un projet ML complet suit toujours : Exploration ‚Üí Preprocessing ‚Üí Entra√Ænement ‚Üí √âvaluation.",
                "Comparer plusieurs mod√®les est essentiel (baseline simple vs mod√®le complexe).",
                "L'interpr√©tation est aussi importante que la performance : comprendre pourquoi le mod√®le pr√©dit."
            ],
            exercises: [
                {
                    id: 'quiz-ml-project',
                    question: "Pourquoi s√©pare-t-on les donn√©es en Train et Test ?",
                    options: ["Pour acc√©l√©rer l'entra√Ænement", "Pour v√©rifier que le mod√®le g√©n√©ralise sur des donn√©es non vues", "Pour √©conomiser de la m√©moire"],
                    correctAnswer: 1,
                    explanation: "Le Test Set permet de mesurer la vraie performance du mod√®le sur des donn√©es qu'il n'a jamais vues pendant l'entra√Ænement, √©vitant ainsi l'overfitting."
                }
            ]
        }
    ]
};
