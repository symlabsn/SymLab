// Cours Applied Data Science - 8 Projets Complets avec Comp√©tences D√©taill√©es
// Bas√© sur le curriculum WorldQuant University Applied Data Science Lab

export const appliedDataScienceProjects = [
    // ==================================================================================
    // PROJECT 1: HOUSING IN MEXICO
    // ==================================================================================
    {
        id: 'housing-mexico',
        number: 1,
        title: 'Housing in Mexico',
        titleFr: 'Immobilier au Mexique',
        country: 'üá≤üáΩ',
        flag: 'Mexico',
        duration: '4-6 heures',
        difficulty: 'D√©butant',
        color: 'emerald',
        icon: 'üè†',
        image: '/images/projects/housing-mexico.jpg',
        description: 'Analysez 21 000 propri√©t√©s pour d√©terminer si les prix immobiliers sont plus influenc√©s par la taille ou la localisation.',
        descriptionEn: 'Analyze 21,000 properties to determine if real estate prices are influenced more by property size or location.',

        // Objectifs d'apprentissage
        objectives: [
            'Importer et nettoyer des donn√©es √† partir de fichiers CSV',
            'Construire des visualisations de donn√©es expressives',
            'Examiner la relation entre deux variables',
            'Calculer et interpr√©ter la corr√©lation'
        ],

        // Comp√©tences techniques d√©taill√©es
        skills: {
            dataEngineering: [
                { name: 'Import CSV', level: 'Fondamental', description: 'Utiliser pandas.read_csv() avec gestion des encodages' },
                { name: 'Nettoyage de donn√©es', level: 'Fondamental', description: 'G√©rer les valeurs manquantes, doublons et outliers' },
                { name: 'S√©lection de colonnes', level: 'Fondamental', description: 'Filtrer et s√©lectionner les features pertinentes' }
            ],
            dataAnalysis: [
                { name: 'Statistiques descriptives', level: 'Fondamental', description: 'Calculer mean, median, std avec pandas' },
                { name: 'Corr√©lation', level: 'Interm√©diaire', description: 'Coefficient de Pearson et interpr√©tation' },
                { name: 'Analyse bi-vari√©e', level: 'Interm√©diaire', description: 'Relation entre prix, taille et localisation' }
            ],
            visualization: [
                { name: 'Histogrammes', level: 'Fondamental', description: 'Distribution des prix avec matplotlib' },
                { name: 'Scatter plots', level: 'Fondamental', description: 'Nuages de points prix vs taille' },
                { name: 'Box plots', level: 'Interm√©diaire', description: 'Comparaison par r√©gion g√©ographique' },
                { name: 'Heatmaps', level: 'Interm√©diaire', description: 'Matrices de corr√©lation avec seaborn' }
            ],
            tools: ['Python', 'Pandas', 'Matplotlib', 'Seaborn', 'Jupyter Notebook']
        },

        // Modules du cours
        modules: [
            {
                id: 1,
                title: 'Acquisition des donn√©es',
                duration: '45 min',
                content: [
                    'T√©l√©charger et charger le dataset immobilier',
                    'Explorer la structure des donn√©es avec .info() et .describe()',
                    'Identifier les types de variables (num√©riques vs cat√©gorielles)'
                ]
            },
            {
                id: 2,
                title: 'Nettoyage et pr√©paration',
                duration: '60 min',
                content: [
                    'Traiter les valeurs manquantes (dropna vs fillna)',
                    'Supprimer les doublons',
                    'Convertir les types de donn√©es',
                    'Cr√©er des features d√©riv√©es (prix/m¬≤)'
                ]
            },
            {
                id: 3,
                title: 'Visualisation exploratoire',
                duration: '90 min',
                content: [
                    'Cr√©er des histogrammes de distribution',
                    'Construire des scatter plots interactifs',
                    'Analyser les box plots par r√©gion',
                    'G√©n√©rer des heatmaps de corr√©lation'
                ]
            },
            {
                id: 4,
                title: 'Analyse et conclusions',
                duration: '45 min',
                content: [
                    'Calculer les coefficients de corr√©lation',
                    'Interpr√©ter les r√©sultats statistiques',
                    'R√©pondre √† la question : Taille vs Localisation ?',
                    'R√©diger un rapport de synth√®se'
                ]
            }
        ],

        // Dataset info
        dataset: {
            source: 'Properati Mexico Real Estate',
            size: '21,000 propri√©t√©s',
            features: ['price', 'surface_total', 'surface_covered', 'lat', 'lon', 'state', 'municipality'],
            format: 'CSV'
        },

        // Code samples
        codeSnippets: [
            {
                title: 'Chargement des donn√©es',
                language: 'python',
                code: `import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Charger le dataset
df = pd.read_csv('mexico_real_estate.csv')

# Aper√ßu des donn√©es
print(df.shape)
df.head()`
            },
            {
                title: 'Calcul de corr√©lation',
                language: 'python',
                code: `# Corr√©lation entre prix et surface
correlation = df['price'].corr(df['surface_total'])
print(f"Corr√©lation prix/surface: {correlation:.3f}")

# Matrice de corr√©lation compl√®te
corr_matrix = df[['price', 'surface_total', 'lat', 'lon']].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')`
            }
        ]
    },

    // ==================================================================================
    // PROJECT 2: APARTMENT SALES IN BUENOS AIRES
    // ==================================================================================
    {
        id: 'apartments-buenos-aires',
        number: 2,
        title: 'Apartment Sales in Buenos Aires',
        titleFr: 'Ventes d\'Appartements √† Buenos Aires',
        country: 'üá¶üá∑',
        flag: 'Argentina',
        duration: '6-8 heures',
        difficulty: 'Interm√©diaire',
        color: 'blue',
        icon: 'üè¢',
        description: 'Construisez un mod√®le de r√©gression lin√©aire pour pr√©dire les prix des appartements en Argentine.',
        descriptionEn: 'Build a linear regression model to predict apartment prices in Argentina.',

        objectives: [
            'Construire un mod√®le de r√©gression lin√©aire',
            'Cr√©er un pipeline de donn√©es pour imputer les valeurs manquantes',
            'Encoder les variables cat√©gorielles',
            'Am√©liorer les performances en r√©duisant l\'overfitting'
        ],

        skills: {
            dataEngineering: [
                { name: 'Pipelines scikit-learn', level: 'Interm√©diaire', description: 'Construire des Pipeline et ColumnTransformer' },
                { name: 'Imputation', level: 'Interm√©diaire', description: 'SimpleImputer pour valeurs manquantes' },
                { name: 'Encodage cat√©goriel', level: 'Interm√©diaire', description: 'OneHotEncoder, LabelEncoder' }
            ],
            machineLearning: [
                { name: 'R√©gression lin√©aire', level: 'Fondamental', description: 'LinearRegression de scikit-learn' },
                { name: 'Train/Test Split', level: 'Fondamental', description: 'S√©paration des donn√©es' },
                { name: 'Cross-validation', level: 'Interm√©diaire', description: 'Validation crois√©e K-fold' },
                { name: 'R√©gularisation', level: 'Interm√©diaire', description: 'Ridge et Lasso pour r√©duire overfitting' }
            ],
            evaluation: [
                { name: 'MSE/RMSE', level: 'Fondamental', description: 'Mean Squared Error' },
                { name: 'R¬≤ Score', level: 'Fondamental', description: 'Coefficient de d√©termination' },
                { name: 'Residual Analysis', level: 'Interm√©diaire', description: 'Analyse des r√©sidus' }
            ],
            tools: ['Python', 'Scikit-learn', 'Pandas', 'NumPy', 'Matplotlib']
        },

        modules: [
            {
                id: 1,
                title: 'Exploration des donn√©es',
                duration: '60 min',
                content: [
                    'Charger le dataset Buenos Aires',
                    'Analyser les distributions de prix',
                    'Identifier les features pr√©dictives'
                ]
            },
            {
                id: 2,
                title: 'Pipeline de pr√©traitement',
                duration: '90 min',
                content: [
                    'Cr√©er un Pipeline scikit-learn',
                    'Impl√©menter SimpleImputer',
                    'Appliquer OneHotEncoder aux cat√©gories',
                    'Combiner avec ColumnTransformer'
                ]
            },
            {
                id: 3,
                title: 'Mod√©lisation',
                duration: '90 min',
                content: [
                    'Entra√Æner LinearRegression',
                    '√âvaluer avec train_test_split',
                    'Interpr√©ter les coefficients',
                    'Visualiser les pr√©dictions vs r√©alit√©'
                ]
            },
            {
                id: 4,
                title: 'Am√©lioration du mod√®le',
                duration: '60 min',
                content: [
                    'D√©tecter l\'overfitting',
                    'Appliquer Ridge Regression',
                    'Comparer les performances',
                    'Choisir le meilleur mod√®le'
                ]
            }
        ],

        dataset: {
            source: 'Properati Argentina',
            size: '15,000+ appartements',
            features: ['price_usd', 'surface_total', 'rooms', 'bathrooms', 'neighborhood', 'property_type'],
            format: 'CSV'
        },

        codeSnippets: [
            {
                title: 'Pipeline de pr√©traitement',
                language: 'python',
                code: `from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# D√©finir les transformations
numeric_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='median'))
])

categorical_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer([
    ('num', numeric_transformer, ['surface_total', 'rooms']),
    ('cat', categorical_transformer, ['neighborhood'])
])`
            }
        ]
    },

    // ==================================================================================
    // PROJECT 3: AIR QUALITY IN NAIROBI
    // ==================================================================================
    {
        id: 'air-quality-nairobi',
        number: 3,
        title: 'Air Quality in Nairobi',
        titleFr: 'Qualit√© de l\'Air √† Nairobi',
        country: 'üá∞üá™',
        flag: 'Kenya',
        duration: '6-8 heures',
        difficulty: 'Interm√©diaire',
        color: 'green',
        icon: 'üåç',
        description: 'Construisez un mod√®le ARMA pour pr√©dire les niveaux de particules fines au Kenya.',
        descriptionEn: 'Build an ARMA time-series model to predict particulate matter levels in Kenya.',

        objectives: [
            'Construire un mod√®le de s√©rie temporelle ARMA',
            'Extraire des donn√©es depuis MongoDB avec pymongo',
            'Am√©liorer les performances par hyperparameter tuning',
            'Analyser et pr√©voir la pollution atmosph√©rique'
        ],

        skills: {
            dataEngineering: [
                { name: 'MongoDB', level: 'Interm√©diaire', description: 'Connexion et requ√™tes avec pymongo' },
                { name: 'S√©ries temporelles', level: 'Interm√©diaire', description: 'Indexation datetime et resampling' },
                { name: 'Feature engineering temporel', level: 'Interm√©diaire', description: 'Lag features, rolling means' }
            ],
            timeSeries: [
                { name: 'Stationnarit√©', level: 'Interm√©diaire', description: 'Test ADF, diff√©renciation' },
                { name: 'ACF/PACF', level: 'Interm√©diaire', description: 'Autocorrelation analysis' },
                { name: 'ARMA/ARIMA', level: 'Avanc√©', description: 'Mod√©lisation AutoRegressive Moving Average' },
                { name: 'Pr√©vision', level: 'Interm√©diaire', description: 'Forecast et intervalles de confiance' }
            ],
            optimization: [
                { name: 'Grid Search', level: 'Interm√©diaire', description: 'Recherche des meilleurs param√®tres p, q' },
                { name: 'AIC/BIC', level: 'Interm√©diaire', description: 'Crit√®res de s√©lection de mod√®le' }
            ],
            tools: ['Python', 'statsmodels', 'pymongo', 'MongoDB', 'Pandas', 'Matplotlib']
        },

        modules: [
            {
                id: 1,
                title: 'Connexion MongoDB',
                duration: '45 min',
                content: [
                    'Configurer pymongo',
                    'Se connecter √† la base de donn√©es',
                    'Extraire les mesures de pollution',
                    'Convertir en DataFrame pandas'
                ]
            },
            {
                id: 2,
                title: 'Analyse des s√©ries temporelles',
                duration: '90 min',
                content: [
                    'Visualiser les tendances PM2.5',
                    'Tester la stationnarit√© (ADF test)',
                    'Appliquer la diff√©renciation si n√©cessaire',
                    'Analyser ACF et PACF'
                ]
            },
            {
                id: 3,
                title: 'Mod√©lisation ARMA',
                duration: '90 min',
                content: [
                    'Choisir les ordres p et q',
                    'Entra√Æner le mod√®le ARMA',
                    '√âvaluer les r√©sidus',
                    'G√©n√©rer des pr√©visions'
                ]
            },
            {
                id: 4,
                title: 'Optimisation',
                duration: '45 min',
                content: [
                    'Impl√©menter Grid Search sur (p, q)',
                    'Comparer AIC/BIC',
                    'S√©lectionner le mod√®le optimal',
                    'Visualiser les pr√©visions finales'
                ]
            }
        ],

        dataset: {
            source: 'Nairobi Air Quality Sensors',
            size: 'Mesures horaires sur 2+ ans',
            features: ['timestamp', 'pm2_5', 'pm10', 'temperature', 'humidity'],
            format: 'MongoDB'
        },

        codeSnippets: [
            {
                title: 'Extraction MongoDB',
                language: 'python',
                code: `from pymongo import MongoClient
import pandas as pd

# Connexion √† MongoDB
client = MongoClient("mongodb://localhost:27017/")
db = client["air_quality"]
collection = db["nairobi"]

# Extraction des donn√©es
cursor = collection.find({}, {"timestamp": 1, "pm2_5": 1})
df = pd.DataFrame(list(cursor))
df['timestamp'] = pd.to_datetime(df['timestamp'])
df.set_index('timestamp', inplace=True)`
            },
            {
                title: 'Mod√®le ARMA',
                language: 'python',
                code: `from statsmodels.tsa.arima.model import ARIMA

# Entra√Æner le mod√®le ARMA(2,1)
model = ARIMA(df['pm2_5'], order=(2, 0, 1))
results = model.fit()

# Pr√©visions sur 24 heures
forecast = results.forecast(steps=24)
print(forecast)`
            }
        ]
    },

    // ==================================================================================
    // PROJECT 4: EARTHQUAKE DAMAGE IN NEPAL
    // ==================================================================================
    {
        id: 'earthquake-nepal',
        number: 4,
        title: 'Earthquake Damage in Nepal',
        titleFr: 'D√©g√¢ts Sismiques au N√©pal',
        country: 'üá≥üáµ',
        flag: 'Nepal',
        duration: '8-10 heures',
        difficulty: 'Interm√©diaire',
        color: 'orange',
        icon: 'üèöÔ∏è',
        description: 'Construisez des mod√®les de r√©gression logistique et arbres de d√©cision pour pr√©dire les d√©g√¢ts des s√©ismes.',
        descriptionEn: 'Build logistic regression and decision tree models to predict earthquake damage to buildings.',

        objectives: [
            'Construire des mod√®les de classification (Logistic Regression, Decision Tree)',
            'Extraire des donn√©es depuis SQLite',
            'R√©v√©ler les biais dans les donn√©es pouvant mener √† la discrimination',
            '√âvaluer l\'√©quit√© des mod√®les'
        ],

        skills: {
            dataEngineering: [
                { name: 'SQLite', level: 'Interm√©diaire', description: 'Requ√™tes SQL avec sqlite3 et pandas' },
                { name: 'JOINs SQL', level: 'Interm√©diaire', description: 'Fusionner plusieurs tables' },
                { name: 'Feature selection', level: 'Interm√©diaire', description: 'Choisir les variables pertinentes' }
            ],
            machineLearning: [
                { name: 'R√©gression logistique', level: 'Interm√©diaire', description: 'Classification binaire et multiclasse' },
                { name: 'Decision Trees', level: 'Interm√©diaire', description: 'Arbres de d√©cision CART' },
                { name: 'Feature importance', level: 'Interm√©diaire', description: 'Identifier les variables cl√©s' }
            ],
            ethics: [
                { name: 'D√©tection de biais', level: 'Avanc√©', description: 'Identifier les discriminations dans les donn√©es' },
                { name: 'Fairness metrics', level: 'Avanc√©', description: 'Demographic parity, Equalized odds' },
                { name: 'Impact assessment', level: 'Avanc√©', description: '√âvaluer les cons√©quences des pr√©dictions' }
            ],
            tools: ['Python', 'Scikit-learn', 'SQLite', 'Pandas', 'Matplotlib']
        },

        modules: [
            {
                id: 1,
                title: 'Extraction SQLite',
                duration: '60 min',
                content: [
                    'Connecter √† la base SQLite',
                    'Explorer le sch√©ma des tables',
                    '√âcrire des requ√™tes SQL avec JOINs',
                    'Charger les donn√©es dans pandas'
                ]
            },
            {
                id: 2,
                title: 'Pr√©paration des donn√©es',
                duration: '90 min',
                content: [
                    'Encoder la cible (damage_grade)',
                    'Traiter les variables g√©ographiques',
                    'Cr√©er des features engineered',
                    '√âquilibrer les classes si n√©cessaire'
                ]
            },
            {
                id: 3,
                title: 'Mod√®les de classification',
                duration: '120 min',
                content: [
                    'Entra√Æner Logistic Regression',
                    'Entra√Æner Decision Tree',
                    'Comparer les performances',
                    'Analyser feature importance'
                ]
            },
            {
                id: 4,
                title: 'Analyse des biais',
                duration: '60 min',
                content: [
                    'Identifier les biais g√©ographiques',
                    'Calculer les m√©triques de fairness',
                    'Discuter les implications √©thiques',
                    'Proposer des corrections'
                ]
            }
        ],

        dataset: {
            source: 'Nepal Earthquake Open Data Portal',
            size: '260,000+ b√¢timents',
            features: ['building_id', 'damage_grade', 'geo_level', 'age', 'area', 'foundation_type', 'roof_type'],
            format: 'SQLite'
        },

        codeSnippets: [
            {
                title: 'Extraction SQLite',
                language: 'python',
                code: `import sqlite3
import pandas as pd

# Connexion √† SQLite
conn = sqlite3.connect('nepal_earthquake.db')

# Requ√™te avec JOIN
query = """
SELECT b.*, d.damage_grade
FROM buildings b
JOIN damage d ON b.building_id = d.building_id
WHERE d.damage_grade IS NOT NULL
"""
df = pd.read_sql(query, conn)
conn.close()`
            }
        ]
    },

    // ==================================================================================
    // PROJECT 5: BANKRUPTCY IN POLAND
    // ==================================================================================
    {
        id: 'bankruptcy-poland',
        number: 5,
        title: 'Bankruptcy in Poland',
        titleFr: 'Faillite d\'Entreprises en Pologne',
        country: 'üáµüá±',
        flag: 'Poland',
        duration: '8-10 heures',
        difficulty: 'Avanc√©',
        color: 'red',
        icon: 'üìâ',
        description: 'Construisez des mod√®les Random Forest et Gradient Boosting pour pr√©dire les faillites d\'entreprises.',
        descriptionEn: 'Build random forest and gradient boosting models to predict whether a company will go bankrupt.',

        objectives: [
            'Construire des mod√®les Random Forest et Gradient Boosting',
            'Naviguer en ligne de commande Linux',
            'G√©rer les donn√©es d√©s√©quilibr√©es par resampling',
            'Comprendre l\'impact de precision et recall'
        ],

        skills: {
            dataEngineering: [
                { name: 'Linux CLI', level: 'Interm√©diaire', description: 'Navigation, manipulation de fichiers' },
                { name: 'Gestion du d√©s√©quilibre', level: 'Avanc√©', description: 'SMOTE, undersampling, class_weight' },
                { name: 'Feature scaling', level: 'Interm√©diaire', description: 'StandardScaler, MinMaxScaler' }
            ],
            machineLearning: [
                { name: 'Random Forest', level: 'Avanc√©', description: 'Ensemble d\'arbres de d√©cision' },
                { name: 'Gradient Boosting', level: 'Avanc√©', description: 'XGBoost, LightGBM concepts' },
                { name: 'Hyperparameter tuning', level: 'Avanc√©', description: 'GridSearchCV, RandomizedSearchCV' }
            ],
            evaluation: [
                { name: 'Precision/Recall', level: 'Avanc√©', description: 'Trade-off et implications business' },
                { name: 'F1-Score', level: 'Interm√©diaire', description: 'Moyenne harmonique' },
                { name: 'ROC-AUC', level: 'Avanc√©', description: 'Courbe ROC et aire sous la courbe' },
                { name: 'Confusion Matrix', level: 'Interm√©diaire', description: 'Analyse d√©taill√©e des erreurs' }
            ],
            tools: ['Python', 'Scikit-learn', 'imbalanced-learn', 'Linux Bash', 'XGBoost']
        },

        modules: [
            {
                id: 1,
                title: 'Linux CLI Basics',
                duration: '60 min',
                content: [
                    'Naviguer avec cd, ls, pwd',
                    'Manipuler les fichiers (cp, mv, rm)',
                    'Lire les fichiers (cat, head, tail)',
                    'Comprendre les permissions'
                ]
            },
            {
                id: 2,
                title: 'Gestion du d√©s√©quilibre',
                duration: '90 min',
                content: [
                    'Analyser le ratio bankruptcy/non-bankruptcy',
                    'Impl√©menter SMOTE',
                    'Utiliser class_weight dans les mod√®les',
                    'Comparer undersampling vs oversampling'
                ]
            },
            {
                id: 3,
                title: 'Mod√®les d\'ensemble',
                duration: '120 min',
                content: [
                    'Entra√Æner Random Forest Classifier',
                    'Entra√Æner Gradient Boosting Classifier',
                    'Tuner les hyperparam√®tres',
                    'Comparer les performances'
                ]
            },
            {
                id: 4,
                title: 'M√©triques m√©tier',
                duration: '60 min',
                content: [
                    'Calculer precision, recall, F1',
                    'Tracer la courbe ROC',
                    'Interpr√©ter pour le contexte financier',
                    'Choisir le seuil optimal'
                ]
            }
        ],

        dataset: {
            source: 'Polish Bankruptcy Dataset',
            size: '10,000+ entreprises',
            features: ['64 ratios financiers', 'bankruptcy (0/1)'],
            format: 'CSV'
        },

        codeSnippets: [
            {
                title: 'SMOTE pour d√©s√©quilibre',
                language: 'python',
                code: `from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

# S√©parer features et target
X = df.drop('bankruptcy', axis=1)
y = df['bankruptcy']

# Appliquer SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

print(f"Avant: {y.value_counts()}")
print(f"Apr√®s: {pd.Series(y_resampled).value_counts()}")`
            }
        ]
    },

    // ==================================================================================
    // PROJECT 6: CUSTOMER SEGMENTATION IN THE US
    // ==================================================================================
    {
        id: 'customer-segmentation-us',
        number: 6,
        title: 'Customer Segmentation in the US',
        titleFr: 'Segmentation Client aux √âtats-Unis',
        country: 'üá∫üá∏',
        flag: 'USA',
        duration: '8-10 heures',
        difficulty: 'Avanc√©',
        color: 'purple',
        icon: 'üë•',
        description: 'Construisez un mod√®le K-Means pour segmenter les consommateurs am√©ricains en groupes.',
        descriptionEn: 'Build a k-means model to cluster US consumers into groups.',

        objectives: [
            'Construire un mod√®le K-Means de clustering',
            'Utiliser PCA pour la r√©duction de dimensionnalit√© et visualisation',
            'Cr√©er un dashboard interactif avec Plotly Dash',
            'Interpr√©ter les segments clients'
        ],

        skills: {
            unsupervisedLearning: [
                { name: 'K-Means', level: 'Avanc√©', description: 'Algorithme de clustering' },
                { name: 'Elbow Method', level: 'Interm√©diaire', description: 'D√©terminer le nombre optimal de clusters' },
                { name: 'Silhouette Score', level: 'Avanc√©', description: '√âvaluer la qualit√© des clusters' }
            ],
            dimensionalityReduction: [
                { name: 'PCA', level: 'Avanc√©', description: 'Analyse en Composantes Principales' },
                { name: 'Variance expliqu√©e', level: 'Avanc√©', description: 'Choisir le nombre de composantes' },
                { name: 'Visualisation 2D/3D', level: 'Interm√©diaire', description: 'Projeter les donn√©es' }
            ],
            webDevelopment: [
                { name: 'Plotly Dash', level: 'Avanc√©', description: 'Framework de dashboards Python' },
                { name: 'Callbacks', level: 'Avanc√©', description: 'Interactivit√© r√©active' },
                { name: 'Layout design', level: 'Interm√©diaire', description: 'Structure de l\'application' }
            ],
            tools: ['Python', 'Scikit-learn', 'Plotly', 'Dash', 'Pandas']
        },

        modules: [
            {
                id: 1,
                title: 'Pr√©paration des donn√©es',
                duration: '60 min',
                content: [
                    'Charger les donn√©es consommateurs',
                    'Normaliser les variables',
                    'S√©lectionner les features pertinentes'
                ]
            },
            {
                id: 2,
                title: 'Clustering K-Means',
                duration: '90 min',
                content: [
                    'Impl√©menter K-Means',
                    'Utiliser la m√©thode du coude',
                    'Calculer le silhouette score',
                    'Assigner les labels aux clients'
                ]
            },
            {
                id: 3,
                title: 'PCA et Visualisation',
                duration: '90 min',
                content: [
                    'Appliquer PCA',
                    'Analyser la variance expliqu√©e',
                    'Visualiser les clusters en 2D',
                    'Interpr√©ter les composantes principales'
                ]
            },
            {
                id: 4,
                title: 'Dashboard Plotly Dash',
                duration: '120 min',
                content: [
                    'Cr√©er la structure de l\'app Dash',
                    'Ajouter des graphiques interactifs',
                    'Impl√©menter des callbacks',
                    'D√©ployer le dashboard'
                ]
            }
        ],

        dataset: {
            source: 'US Consumer Survey',
            size: '50,000+ consommateurs',
            features: ['demographics', 'spending_habits', 'preferences', 'income'],
            format: 'CSV'
        },

        codeSnippets: [
            {
                title: 'K-Means et Elbow',
                language: 'python',
                code: `from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# M√©thode du coude
inertias = []
K_range = range(2, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)

# Mod√®le final
kmeans = KMeans(n_clusters=5, random_state=42)
clusters = kmeans.fit_predict(X_scaled)`
            },
            {
                title: 'Dashboard Dash',
                language: 'python',
                code: `import dash
from dash import dcc, html
import plotly.express as px

app = dash.Dash(__name__)

app.layout = html.Div([
    html.H1('Customer Segmentation Dashboard'),
    dcc.Graph(
        id='cluster-plot',
        figure=px.scatter(df, x='PC1', y='PC2', 
                         color='cluster', 
                         title='Customer Clusters')
    )
])

if __name__ == '__main__':
    app.run_server(debug=True)`
            }
        ]
    },

    // ==================================================================================
    // PROJECT 7: A/B TESTING AT WORLDQUANT UNIVERSITY
    // ==================================================================================
    {
        id: 'ab-testing-wqu',
        number: 7,
        title: 'A/B Testing at WorldQuant University',
        titleFr: 'Test A/B √† WorldQuant University',
        country: 'üåê',
        flag: 'Global',
        duration: '8-10 heures',
        difficulty: 'Avanc√©',
        color: 'indigo',
        icon: 'üìß',
        description: 'Menez un test Chi-carr√© pour d√©terminer si l\'envoi d\'emails augmente les inscriptions.',
        descriptionEn: 'Conduct a chi-square test to determine if sending an email can increase program enrollment at WQU.',

        objectives: [
            'Mener un test statistique Chi-carr√©',
            'Construire des classes Python personnalis√©es pour ETL',
            'Cr√©er une application interactive avec design three-tiered',
            'Interpr√©ter les r√©sultats statistiques'
        ],

        skills: {
            statistics: [
                { name: 'Test Chi-carr√©', level: 'Avanc√©', description: 'Test d\'ind√©pendance statistique' },
                { name: 'Hypoth√®ses H0/H1', level: 'Interm√©diaire', description: 'Formulation et test' },
                { name: 'P-value', level: 'Interm√©diaire', description: 'Interpr√©tation et seuil de significativit√©' }
            ],
            softwareEngineering: [
                { name: 'Classes Python', level: 'Avanc√©', description: 'OOP, \\__init__, m√©thodes' },
                { name: 'ETL Pipeline', level: 'Avanc√©', description: 'Extract, Transform, Load' },
                { name: 'Design patterns', level: 'Avanc√©', description: 'Three-tiered architecture' }
            ],
            webDevelopment: [
                { name: 'Data layer', level: 'Avanc√©', description: 'Acc√®s aux donn√©es' },
                { name: 'Business layer', level: 'Avanc√©', description: 'Logique m√©tier' },
                { name: 'Presentation layer', level: 'Avanc√©', description: 'Interface utilisateur' }
            ],
            tools: ['Python', 'SciPy', 'OOP', 'Flask/Dash', 'SQLite']
        },

        modules: [
            {
                id: 1,
                title: 'Design de l\'exp√©rience',
                duration: '60 min',
                content: [
                    'D√©finir les groupes contr√¥le et test',
                    'Formuler les hypoth√®ses',
                    'Calculer la taille d\'√©chantillon requise'
                ]
            },
            {
                id: 2,
                title: 'Classes ETL Python',
                duration: '120 min',
                content: [
                    'Cr√©er une classe Extractor',
                    'Cr√©er une classe Transformer',
                    'Cr√©er une classe Loader',
                    'Orchestrer le pipeline'
                ]
            },
            {
                id: 3,
                title: 'Analyse statistique',
                duration: '90 min',
                content: [
                    'Construire la table de contingence',
                    'Effectuer le test Chi-carr√©',
                    'Interpr√©ter la p-value',
                    'Conclure sur l\'efficacit√© de l\'email'
                ]
            },
            {
                id: 4,
                title: 'Application Three-Tiered',
                duration: '90 min',
                content: [
                    'Impl√©menter la couche donn√©es',
                    'Impl√©menter la couche m√©tier',
                    'Cr√©er l\'interface de pr√©sentation',
                    'Int√©grer les trois couches'
                ]
            }
        ],

        dataset: {
            source: 'WQU Enrollment Experiment',
            size: '10,000+ prospects',
            features: ['user_id', 'email_sent', 'enrolled', 'timestamp'],
            format: 'SQLite'
        },

        codeSnippets: [
            {
                title: 'Test Chi-carr√©',
                language: 'python',
                code: `from scipy.stats import chi2_contingency
import pandas as pd

# Table de contingence
contingency = pd.crosstab(df['email_sent'], df['enrolled'])

# Test Chi-carr√©
chi2, p_value, dof, expected = chi2_contingency(contingency)

print(f"Chi¬≤ statistic: {chi2:.4f}")
print(f"P-value: {p_value:.4f}")
print(f"Significatif (Œ±=0.05): {p_value < 0.05}")`
            },
            {
                title: 'Classe ETL',
                language: 'python',
                code: `class DataExtractor:
    def __init__(self, db_path):
        self.db_path = db_path
    
    def extract(self, query):
        import sqlite3
        conn = sqlite3.connect(self.db_path)
        df = pd.read_sql(query, conn)
        conn.close()
        return df

class DataTransformer:
    def transform(self, df):
        # Nettoyage et transformation
        df['enrolled'] = df['enrolled'].astype(int)
        return df`
            }
        ]
    },

    // ==================================================================================
    // PROJECT 8: VOLATILITY FORECASTING IN INDIA
    // ==================================================================================
    {
        id: 'volatility-india',
        number: 8,
        title: 'Volatility Forecasting in India',
        titleFr: 'Pr√©vision de Volatilit√© en Inde',
        country: 'üáÆüá≥',
        flag: 'India',
        duration: '10-12 heures',
        difficulty: 'Expert',
        color: 'amber',
        icon: 'üìà',
        description: 'Cr√©ez un mod√®le GARCH pour pr√©dire la volatilit√© des actifs et construisez votre propre API.',
        descriptionEn: 'Create a GARCH time series model to predict asset volatility.',

        objectives: [
            'Cr√©er un mod√®le de s√©rie temporelle GARCH',
            'Acqu√©rir des donn√©es via API',
            'Nettoyer et stocker dans SQLite',
            'Construire une API pour servir les pr√©dictions'
        ],

        skills: {
            timeSeries: [
                { name: 'GARCH', level: 'Expert', description: 'Generalized Autoregressive Conditional Heteroskedasticity' },
                { name: 'Volatilit√©', level: 'Avanc√©', description: 'Mesure et pr√©vision de la volatilit√©' },
                { name: 'Returns', level: 'Interm√©diaire', description: 'Calcul des rendements log' }
            ],
            dataEngineering: [
                { name: 'API consumption', level: 'Avanc√©', description: 'Requests, authentification, pagination' },
                { name: 'SQLite storage', level: 'Interm√©diaire', description: 'Stockage et requ√™tes' },
                { name: 'Data pipeline', level: 'Avanc√©', description: 'Automatisation de l\'ingestion' }
            ],
            apiDevelopment: [
                { name: 'FastAPI/Flask', level: 'Avanc√©', description: 'Cr√©ation d\'endpoints REST' },
                { name: 'JSON serialization', level: 'Interm√©diaire', description: 'Format de r√©ponse' },
                { name: 'Documentation', level: 'Interm√©diaire', description: 'OpenAPI/Swagger' }
            ],
            tools: ['Python', 'arch', 'FastAPI', 'SQLite', 'yfinance', 'Requests']
        },

        modules: [
            {
                id: 1,
                title: 'Acquisition via API',
                duration: '90 min',
                content: [
                    'Utiliser yfinance pour les donn√©es boursi√®res',
                    'G√©rer les limites de rate',
                    'Nettoyer les donn√©es temporelles',
                    'Stocker dans SQLite'
                ]
            },
            {
                id: 2,
                title: 'Analyse de volatilit√©',
                duration: '90 min',
                content: [
                    'Calculer les rendements logarithmiques',
                    'Visualiser la volatilit√© historique',
                    'Tester les effets ARCH',
                    'Analyser le clustering de volatilit√©'
                ]
            },
            {
                id: 3,
                title: 'Mod√©lisation GARCH',
                duration: '120 min',
                content: [
                    'Entra√Æner un mod√®le GARCH(1,1)',
                    'Interpr√©ter les param√®tres Œ± et Œ≤',
                    'G√©n√©rer des pr√©visions de volatilit√©',
                    '√âvaluer la qualit√© du mod√®le'
                ]
            },
            {
                id: 4,
                title: 'API de pr√©diction',
                duration: '120 min',
                content: [
                    'Cr√©er une API FastAPI',
                    'Impl√©menter endpoint /predict',
                    'S√©rialiser les pr√©dictions en JSON',
                    'Documenter avec Swagger',
                    'D√©ployer l\'API'
                ]
            }
        ],

        dataset: {
            source: 'Yahoo Finance API (Nifty 50)',
            size: '5+ ann√©es de donn√©es journali√®res',
            features: ['date', 'open', 'high', 'low', 'close', 'volume'],
            format: 'API ‚Üí SQLite'
        },

        codeSnippets: [
            {
                title: 'Mod√®le GARCH',
                language: 'python',
                code: `from arch import arch_model
import yfinance as yf
import numpy as np

# T√©l√©charger les donn√©es
data = yf.download('^NSEI', start='2018-01-01')
returns = 100 * np.log(data['Close'] / data['Close'].shift(1)).dropna()

# Mod√®le GARCH(1,1)
model = arch_model(returns, vol='Garch', p=1, q=1)
results = model.fit(disp='off')

# Pr√©vision de volatilit√©
forecast = results.forecast(horizon=5)
print(forecast.variance.iloc[-1])`
            },
            {
                title: 'API FastAPI',
                language: 'python',
                code: `from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Volatility Forecast API")

class ForecastResponse(BaseModel):
    ticker: str
    horizon: int
    volatility: list

@app.get("/predict/{ticker}")
def predict_volatility(ticker: str, horizon: int = 5):
    # Charger mod√®le et pr√©dire
    volatility = get_garch_forecast(ticker, horizon)
    return ForecastResponse(
        ticker=ticker,
        horizon=horizon,
        volatility=volatility
    )`
            }
        ]
    }
];

// Fonction pour obtenir un projet par ID
export const getProjectById = (id) => {
    return appliedDataScienceProjects.find(p => p.id === id);
};

// Fonction pour obtenir les projets par difficult√©
export const getProjectsByDifficulty = (difficulty) => {
    return appliedDataScienceProjects.filter(p => p.difficulty === difficulty);
};

// Statistiques globales
export const projectsStats = {
    total: appliedDataScienceProjects.length,
    countries: [...new Set(appliedDataScienceProjects.map(p => p.flag))].length,
    totalHours: '60-80 heures',
    skills: ['Python', 'Pandas', 'Scikit-learn', 'SQL', 'APIs', 'Dashboards', 'Time Series', 'Machine Learning']
};
