// Data Science Appliqu√©e - 8 Projets Complets avec Contenu D√©taill√©
// Curriculum inspir√© de WorldQuant University Applied Data Science Lab

export const dataScienceProjects = [
    // ==================================================================================
    // PROJET 1: IMMOBILIER AU MEXIQUE
    // ==================================================================================
    {
        id: 'housing-mexico',
        numero: 1,
        titre: 'Immobilier au Mexique',
        pays: 'üá≤üáΩ Mexique',
        duree: '4-6 heures',
        difficulte: 'D√©butant',
        couleur: 'emerald',
        icone: 'üè†',

        resume: 'Analysez un dataset de 21 000 propri√©t√©s immobili√®res mexicaines pour d√©terminer si les prix sont plus influenc√©s par la taille ou par la localisation g√©ographique.',

        // OBJECTIFS D'APPRENTISSAGE
        objectifs: [
            'Importer et nettoyer des donn√©es depuis un fichier CSV',
            'Construire des visualisations de donn√©es percutantes',
            'Examiner la relation entre deux variables quantitatives',
            'Calculer et interpr√©ter le coefficient de corr√©lation de Pearson'
        ],

        // COMP√âTENCES D√âTAILL√âES
        competences: {
            ingenierieDonnees: [
                {
                    nom: 'Import CSV avec Pandas',
                    niveau: 'Fondamental',
                    description: 'Utiliser pd.read_csv() avec gestion des encodages (UTF-8, Latin-1), s√©parateurs et types de donn√©es',
                    concepts: ['pd.read_csv()', 'encoding', 'sep', 'dtype', 'parse_dates']
                },
                {
                    nom: 'Nettoyage de donn√©es',
                    niveau: 'Fondamental',
                    description: 'Identifier et traiter les valeurs manquantes, doublons et outliers',
                    concepts: ['isna()', 'dropna()', 'fillna()', 'drop_duplicates()', 'IQR method']
                },
                {
                    nom: 'S√©lection et filtrage',
                    niveau: 'Fondamental',
                    description: 'S√©lectionner des colonnes et lignes avec loc, iloc et conditions bool√©ennes',
                    concepts: ['df.loc[]', 'df.iloc[]', 'boolean indexing', 'query()']
                }
            ],
            analyseDonnees: [
                {
                    nom: 'Statistiques descriptives',
                    niveau: 'Fondamental',
                    description: 'Calculer moyenne, m√©diane, √©cart-type et percentiles',
                    concepts: ['mean()', 'median()', 'std()', 'describe()', 'quantile()']
                },
                {
                    nom: 'Corr√©lation de Pearson',
                    niveau: 'Interm√©diaire',
                    description: 'Mesurer la force et la direction de la relation lin√©aire entre deux variables',
                    concepts: ['corr()', 'coefficient r', 'interpr√©tation', 'p-value']
                },
                {
                    nom: 'Analyse bi-vari√©e',
                    niveau: 'Interm√©diaire',
                    description: '√âtudier la relation entre prix, surface et localisation',
                    concepts: ['scatter plot', 'groupby', 'agr√©gation', 'pivot_table']
                }
            ],
            visualisation: [
                {
                    nom: 'Histogrammes',
                    niveau: 'Fondamental',
                    description: 'Visualiser la distribution des prix immobiliers',
                    concepts: ['plt.hist()', 'bins', 'density', 'alpha']
                },
                {
                    nom: 'Nuages de points (Scatter plots)',
                    niveau: 'Fondamental',
                    description: 'Repr√©senter la relation prix vs surface',
                    concepts: ['plt.scatter()', 'color', 'size', 'alpha', 'regression line']
                },
                {
                    nom: 'Bo√Ætes √† moustaches (Box plots)',
                    niveau: 'Interm√©diaire',
                    description: 'Comparer les distributions par r√©gion g√©ographique',
                    concepts: ['sns.boxplot()', 'IQR', 'outliers', 'comparaison de groupes']
                },
                {
                    nom: 'Cartes de chaleur (Heatmaps)',
                    niveau: 'Interm√©diaire',
                    description: 'Visualiser les matrices de corr√©lation',
                    concepts: ['sns.heatmap()', 'annot', 'cmap', 'mask', 'correlation matrix']
                }
            ]
        },

        // MODULES DU COURS
        modules: [
            {
                id: 1,
                titre: 'Acquisition et exploration des donn√©es',
                duree: '45 minutes',
                objectif: 'Charger le dataset et comprendre sa structure',
                contenu: [
                    {
                        type: 'theorie',
                        titre: 'Introduction aux donn√©es immobili√®res',
                        texte: `Le march√© immobilier mexicain est l'un des plus dynamiques d'Am√©rique Latine. 
                        Notre dataset contient 21 000 propri√©t√©s avec leurs caract√©ristiques principales :
                        - Prix en USD
                        - Surface totale et couverte (m¬≤)
                        - Coordonn√©es g√©ographiques (latitude, longitude)
                        - √âtat et municipalit√©
                        
                        Question cl√© : Le prix d√©pend-il plus de la TAILLE ou de la LOCALISATION ?`
                    },
                    {
                        type: 'code',
                        titre: 'Chargement du dataset',
                        langage: 'python',
                        code: `import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Configuration
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette('husl')

# Chargement des donn√©es
df = pd.read_csv('mexico_real_estate.csv')

# Aper√ßu initial
print(f"Dimensions: {df.shape[0]} lignes √ó {df.shape[1]} colonnes")
print(f"\\nColonnes: {df.columns.tolist()}")
df.head()`
                    },
                    {
                        type: 'code',
                        titre: 'Exploration de la structure',
                        langage: 'python',
                        code: `# Types de donn√©es et valeurs manquantes
print(df.info())

# Statistiques descriptives
df.describe()

# Valeurs manquantes par colonne
df.isna().sum().sort_values(ascending=False)`
                    },
                    {
                        type: 'exercice',
                        titre: 'Exercice 1.1',
                        enonce: 'Combien de propri√©t√©s ont un prix sup√©rieur √† 500 000 USD ?',
                        indice: 'Utilisez une condition bool√©enne avec df[df["price"] > 500000]',
                        solution: 'len(df[df["price"] > 500000])'
                    }
                ]
            },
            {
                id: 2,
                titre: 'Nettoyage et pr√©paration des donn√©es',
                duree: '60 minutes',
                objectif: 'Pr√©parer les donn√©es pour l\'analyse',
                contenu: [
                    {
                        type: 'theorie',
                        titre: 'Strat√©gies de nettoyage',
                        texte: `Le nettoyage des donn√©es est crucial pour obtenir des r√©sultats fiables.
                        
                        Probl√®mes courants √† traiter :
                        1. Valeurs manquantes (NaN) - supprimer ou imputer ?
                        2. Doublons - propri√©t√©s list√©es plusieurs fois
                        3. Outliers - prix irr√©alistes (erreurs de saisie)
                        4. Types incorrects - colonnes num√©riques stock√©es comme texte
                        
                        R√®gle d'or : Documenter chaque transformation !`
                    },
                    {
                        type: 'code',
                        titre: 'Traitement des valeurs manquantes',
                        langage: 'python',
                        code: `# Visualiser les valeurs manquantes
import missingno as msno
msno.matrix(df)
plt.title("Matrice des valeurs manquantes")
plt.show()

# Option 1: Supprimer les lignes avec valeurs manquantes
df_clean = df.dropna(subset=['price', 'surface_total'])

# Option 2: Imputer avec la m√©diane (pour les surfaces)
df['surface_total'].fillna(df['surface_total'].median(), inplace=True)

print(f"Lignes avant: {len(df)}, apr√®s: {len(df_clean)}")`
                    },
                    {
                        type: 'code',
                        titre: 'D√©tection et traitement des outliers',
                        langage: 'python',
                        code: `# M√©thode IQR pour d√©tecter les outliers
Q1 = df['price'].quantile(0.25)
Q3 = df['price'].quantile(0.75)
IQR = Q3 - Q1

# Bornes
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filtrer les outliers
df_no_outliers = df[(df['price'] >= lower_bound) & (df['price'] <= upper_bound)]

print(f"Outliers supprim√©s: {len(df) - len(df_no_outliers)}")`
                    },
                    {
                        type: 'code',
                        titre: 'Cr√©ation de features d√©riv√©es',
                        langage: 'python',
                        code: `# Prix au m√®tre carr√©
df['price_per_m2'] = df['price'] / df['surface_total']

# Ratio surface couverte / surface totale
df['coverage_ratio'] = df['surface_covered'] / df['surface_total']

# V√©rification
df[['price', 'surface_total', 'price_per_m2', 'coverage_ratio']].describe()`
                    }
                ]
            },
            {
                id: 3,
                titre: 'Visualisation exploratoire',
                duree: '90 minutes',
                objectif: 'Cr√©er des graphiques pour comprendre les donn√©es',
                contenu: [
                    {
                        type: 'code',
                        titre: 'Distribution des prix',
                        langage: 'python',
                        code: `fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Histogramme
axes[0].hist(df['price'], bins=50, color='steelblue', edgecolor='white', alpha=0.7)
axes[0].set_xlabel('Prix (USD)')
axes[0].set_ylabel('Fr√©quence')
axes[0].set_title('Distribution des prix immobiliers au Mexique')
axes[0].axvline(df['price'].median(), color='red', linestyle='--', label=f'M√©diane: ${df["price"].median():, .0f}')
axes[0].legend()

# Distribution log
axes[1].hist(np.log10(df['price']), bins = 50, color = 'coral', edgecolor = 'white', alpha = 0.7)
axes[1].set_xlabel('log‚ÇÅ‚ÇÄ(Prix)')
axes[1].set_ylabel('Fr√©quence')
axes[1].set_title('Distribution logarithmique des prix')

plt.tight_layout()
plt.show()`
                    },
                    {
                        type: 'code',
                        titre: 'Relation Prix vs Surface',
                        langage: 'python',
                        code: `plt.figure(figsize = (10, 6))

# Scatter plot avec r√©gression
sns.regplot(
                            data = df,
                            x = 'surface_total',
                            y = 'price',
                            scatter_kws = { 'alpha': 0.3, 's': 10 },
                            line_kws = { 'color': 'red' }
                        )

plt.xlabel('Surface totale (m¬≤)')
plt.ylabel('Prix (USD)')
plt.title('Relation entre la surface et le prix')

# Ajouter le coefficient de corr√©lation
r = df['price'].corr(df['surface_total'])
plt.annotate(f'r = {r:.3f}', xy = (0.05, 0.95), xycoords = 'axes fraction',
                            fontsize = 14, bbox = dict(boxstyle = 'round', facecolor = 'wheat'))

plt.show()`
                    },
                    {
                        type: 'code',
                        titre: 'Comparaison par r√©gion',
                        langage: 'python',
                        code: `# Top 10 des √©tats par nombre de propri√©t√©s
top_states = df['state'].value_counts().head(10).index

plt.figure(figsize = (12, 6))
sns.boxplot(
                                data = df[df['state'].isin(top_states)],
                                x = 'state',
                                y = 'price',
                                palette = 'viridis'
                            )
plt.xticks(rotation = 45, ha = 'right')
plt.xlabel('√âtat')
plt.ylabel('Prix (USD)')
plt.title('Distribution des prix par √âtat (Top 10)')
plt.tight_layout()
plt.show()`
                    },
                    {
                        type: 'code',
                        titre: 'Matrice de corr√©lation',
                        langage: 'python',
                        code: `# S√©lection des variables num√©riques
numeric_cols = ['price', 'surface_total', 'surface_covered', 'lat', 'lon']
corr_matrix = df[numeric_cols].corr()

# Heatmap
plt.figure(figsize = (8, 6))
sns.heatmap(
                                corr_matrix,
                                annot = True,
                                cmap = 'RdYlBu_r',
                                center = 0,
                                fmt = '.2f',
                                square = True,
                                linewidths = 0.5
                            )
plt.title('Matrice de corr√©lation')
plt.show()`
                    }
                ]
            },
            {
                id: 4,
                titre: 'Analyse et conclusions',
                duree: '45 minutes',
                objectif: 'R√©pondre √† la question initiale avec des preuves',
                contenu: [
                    {
                        type: 'code',
                        titre: 'Analyse de corr√©lation finale',
                        langage: 'python',
                        code: `# Corr√©lations avec le prix
correlations = df[['price', 'surface_total', 'lat', 'lon']].corr()['price'].drop('price')
correlations = correlations.abs().sort_values(ascending = False)

print("Corr√©lations avec le prix (valeur absolue):")
print(correlations)

# Interpr√©tation
print(f"""
CONCLUSION:
                                -----------
                            Corr√©lation Prix - Surface: { df['price'].corr(df['surface_total']): .3f }
Corr√©lation Prix - Latitude: { df['price'].corr(df['lat']): .3f }
Corr√©lation Prix - Longitude: { df['price'].corr(df['lon']): .3f }

La TAILLE(surface) a une corr√©lation plus forte avec le prix
que la localisation g√©ographique(lat / lon).
""")`
                    },
    {
        type: 'code',
        titre: 'Visualisation de la conclusion',
        langage: 'python',
        code: `fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Corr√©lation par facteur
factors = ['Surface', 'Latitude', 'Longitude']
correlations = [0.65, 0.12, 0.08]  # Valeurs exemple
colors = ['green' if c > 0.5 else 'orange' if c > 0.2 else 'red' for c in correlations]

axes[0].barh(factors, correlations, color=colors)
axes[0].set_xlabel('Corr√©lation avec le prix')
axes[0].set_title('Facteurs influen√ßant le prix')
axes[0].axvline(0.5, color='gray', linestyle='--', alpha=0.5)

# Prix moyen par r√©gion
price_by_state = df.groupby('state')['price'].mean().sort_values(ascending=False).head(10)
axes[1].barh(price_by_state.index, price_by_state.values, color='steelblue')
axes[1].set_xlabel('Prix moyen (USD)')
axes[1].set_title('Prix moyen par √âtat (Top 10)')

plt.tight_layout()
plt.show()`
    },
    {
        type: 'exercice',
        titre: 'Projet final',
        enonce: 'R√©digez un rapport de 500 mots r√©sumant vos d√©couvertes. Incluez: (1) La question de recherche, (2) La m√©thodologie, (3) Les r√©sultats cl√©s, (4) Les limites de l\'analyse.',
        indice: 'Structurez votre rapport comme un article scientifique'
    }
]
            }
        ],

// DATASET
dataset: {
    source: 'Properati Mexico Real Estate',
        taille: '21 000 propri√©t√©s',
            colonnes: ['price', 'surface_total', 'surface_covered', 'lat', 'lon', 'state', 'municipality'],
                format: 'CSV',
                    lien: 'https://example.com/mexico_real_estate.csv'
},

// OUTILS
outils: ['Python 3.x', 'Pandas', 'NumPy', 'Matplotlib', 'Seaborn', 'Jupyter Notebook'],

    // PR√âREQUIS
    prerequis: [
        'Bases de Python (variables, boucles, fonctions)',
        'Notions de statistiques descriptives',
        'Installation d\'Anaconda ou Python + pip'
    ]
    },

// ==================================================================================
// PROJET 2: APPARTEMENTS √Ä BUENOS AIRES
// ==================================================================================
{
    id: 'apartments-buenos-aires',
        numero: 2,
            titre: 'Appartements √† Buenos Aires',
                pays: 'üá¶üá∑ Argentine',
                    duree: '6-8 heures',
                        difficulte: 'Interm√©diaire',
                            couleur: 'blue',
                                icone: 'üè¢',

                                    resume: 'Construisez un mod√®le de r√©gression lin√©aire pour pr√©dire les prix des appartements en Argentine. Apprenez √† cr√©er des pipelines de donn√©es et √† r√©duire l\'overfitting.',

                                        objectifs: [
                                            'Construire un mod√®le de r√©gression lin√©aire avec scikit-learn',
                                            'Cr√©er un pipeline de pr√©traitement des donn√©es',
                                            'Imputer les valeurs manquantes automatiquement',
                                            'Encoder les variables cat√©gorielles',
                                            'D√©tecter et r√©duire l\'overfitting avec la r√©gularisation'
                                        ],

                                            competences: {
        ingenierieDonnees: [
            {
                nom: 'Pipelines scikit-learn',
                niveau: 'Interm√©diaire',
                description: 'Cha√Æner les transformations de donn√©es',
                concepts: ['Pipeline', 'ColumnTransformer', 'make_pipeline']
            },
            {
                nom: 'Imputation automatique',
                niveau: 'Interm√©diaire',
                description: 'Remplacer les valeurs manquantes',
                concepts: ['SimpleImputer', 'strategy', 'median', 'most_frequent']
            },
            {
                nom: 'Encodage cat√©goriel',
                niveau: 'Interm√©diaire',
                description: 'Transformer les cat√©gories en nombres',
                concepts: ['OneHotEncoder', 'LabelEncoder', 'handle_unknown']
            }
        ],
            machineLearning: [
                {
                    nom: 'R√©gression lin√©aire',
                    niveau: 'Fondamental',
                    description: 'Mod√©liser la relation lin√©aire entre features et target',
                    concepts: ['LinearRegression', 'fit()', 'predict()', 'coef_', 'intercept_']
                },
                {
                    nom: 'Train/Test Split',
                    niveau: 'Fondamental',
                    description: 'S√©parer les donn√©es pour √©valuer le mod√®le',
                    concepts: ['train_test_split', 'test_size', 'random_state', 'stratify']
                },
                {
                    nom: 'Validation crois√©e',
                    niveau: 'Interm√©diaire',
                    description: '√âvaluation robuste avec K-fold',
                    concepts: ['cross_val_score', 'KFold', 'scoring', 'cv']
                },
                {
                    nom: 'R√©gularisation',
                    niveau: 'Interm√©diaire',
                    description: 'R√©duire l\'overfitting avec Ridge et Lasso',
                    concepts: ['Ridge', 'Lasso', 'alpha', 'L1', 'L2', 'ElasticNet']
                }
            ],
                evaluation: [
                    {
                        nom: 'MSE et RMSE',
                        niveau: 'Fondamental',
                        description: 'Mesurer l\'erreur moyenne de pr√©diction',
                        concepts: ['mean_squared_error', 'RMSE = ‚àöMSE', 'interpr√©tation']
                    },
                    {
                        nom: 'R¬≤ Score',
                        niveau: 'Fondamental',
                        description: 'Coefficient de d√©termination',
                        concepts: ['r2_score', 'variance expliqu√©e', '0 ‚â§ R¬≤ ‚â§ 1']
                    },
                    {
                        nom: 'Analyse des r√©sidus',
                        niveau: 'Interm√©diaire',
                        description: 'V√©rifier les hypoth√®ses du mod√®le',
                        concepts: ['residuals plot', 'normalit√©', 'homosc√©dasticit√©']
                    }
                ]
    },

    modules: [
        {
            id: 1,
            titre: 'Exploration et pr√©paration',
            duree: '90 minutes',
            objectif: 'Comprendre les donn√©es et identifier les transformations n√©cessaires',
            contenu: [
                {
                    type: 'code',
                    titre: 'Chargement et exploration',
                    langage: 'python',
                    code: `import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score

# Charger les donn√©es
df = pd.read_csv('buenos_aires_apartments.csv')

# Exploration
print(df.info())
print(df.describe())

# Variables cat√©gorielles vs num√©riques
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
numerical_cols = df.select_dtypes(include=['number']).columns.tolist()

print(f"Cat√©gorielles: {categorical_cols}")
print(f"Num√©riques: {numerical_cols}")`
                }
            ]
        },
        {
            id: 2,
            titre: 'Construction du Pipeline',
            duree: '90 minutes',
            objectif: 'Cr√©er un pipeline de pr√©traitement automatis√©',
            contenu: [
                {
                    type: 'theorie',
                    titre: 'Pourquoi utiliser des Pipelines ?',
                    texte: `Les pipelines scikit-learn offrent plusieurs avantages :
                        
                        1. **Reproductibilit√©** : M√™mes transformations appliqu√©es uniform√©ment
                        2. **Pr√©vention des fuites** : Pas de data leakage entre train et test
                        3. **Validation crois√©e** : Transformations int√©gr√©es dans le CV
                        4. **D√©ploiement** : Un seul objet √† sauvegarder
                        
                        Structure d'un pipeline :
                        Pipeline = [Pr√©processeur ‚Üí Mod√®le]`
                },
                {
                    type: 'code',
                    titre: 'Pipeline complet',
                    langage: 'python',
                    code: `from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# D√©finir les colonnes
numerical_features = ['surface_total', 'rooms', 'bathrooms']
categorical_features = ['neighborhood', 'property_type']

# Transformateur num√©rique
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Transformateur cat√©goriel
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

# Combiner les transformateurs
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# Pipeline complet avec mod√®le
model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])`
                }
            ]
        },
        {
            id: 3,
            titre: 'Entra√Ænement et √©valuation',
            duree: '90 minutes',
            objectif: 'Entra√Æner le mod√®le et √©valuer ses performances',
            contenu: [
                {
                    type: 'code',
                    titre: 'Entra√Ænement',
                    langage: 'python',
                    code: `# S√©paration features / target
X = df.drop('price_usd', axis=1)
y = df['price_usd']

# Split train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Entra√Ænement
model_pipeline.fit(X_train, y_train)

# Pr√©dictions
y_train_pred = model_pipeline.predict(X_train)
y_test_pred = model_pipeline.predict(X_test)

# M√©triques
print("=== Performances ===")
print(f"Train R¬≤: {r2_score(y_train, y_train_pred):.4f}")
print(f"Test R¬≤: {r2_score(y_test, y_test_pred):.4f}")
print(f"Train RMSE: ${np.sqrt(mean_squared_error(y_train, y_train_pred)):, .0f}")
print(f"Test RMSE: ${np.sqrt(mean_squared_error(y_test, y_test_pred)):,.0f}")`
                    }
                ]
            },
            {
                id: 4,
                titre: 'R√©duction de l\'overfitting',
                duree: '60 minutes',
                objectif: 'Am√©liorer la g√©n√©ralisation avec la r√©gularisation',
                contenu: [
                    {
                        type: 'theorie',
                        titre: 'Overfitting et r√©gularisation',
                        texte: `L'overfitting se produit quand le mod√®le apprend le "bruit" des donn√©es d'entra√Ænement.

            Sympt√¥me : Train R¬≤ >> Test R¬≤

        Solutions :
        - Ridge(L2) : P√©nalise les grands coefficients ‚Üí les r√©duit
        - Lasso(L1) : P√©nalise les coefficients ‚Üí certains deviennent 0(feature selection)
                        
                        Hyperparam√®tre alpha : Plus alpha est grand, plus la r√©gularisation est forte.`
                    },
                    {
                        type: 'code',
                        titre: 'Comparaison des mod√®les',
                        langage: 'python',
                        code: `from sklearn.model_selection import cross_val_score

models = {
        'LinearRegression': LinearRegression(),
            'Ridge (Œ±=1)': Ridge(alpha = 1),
                'Ridge (Œ±=10)': Ridge(alpha = 10),
                    'Lasso (Œ±=1)': Lasso(alpha = 1),
}

    results = []
    for name, model in models.items():
        pipeline = Pipeline([
            ('preprocessor', preprocessor),
            ('regressor', model)
        ])

    scores = cross_val_score(pipeline, X_train, y_train, cv = 5, scoring = 'r2')
    results.append({
        'Mod√®le': name,
        'R¬≤ moyen': scores.mean(),
        '√âcart-type': scores.std()
    })

    pd.DataFrame(results).sort_values('R¬≤ moyen', ascending = False)`
                    }
                ]
            }
        ],
        
        dataset: {
            source: 'Properati Argentina',
            taille: '15 000+ appartements',
            colonnes: ['price_usd', 'surface_total', 'rooms', 'bathrooms', 'neighborhood', 'property_type'],
            format: 'CSV'
        },
        
        outils: ['Python', 'Scikit-learn', 'Pandas', 'NumPy', 'Matplotlib'],
        
        prerequis: [
            'Projet 1 compl√©t√©',
            'Notions de r√©gression lin√©aire',
            'Compr√©hension des concepts train/test'
        ]
    },

    // ==================================================================================
    // PROJET 3: QUALIT√â DE L'AIR √Ä NAIROBI
    // ==================================================================================
    {
        id: 'air-quality-nairobi',
        numero: 3,
        titre: 'Qualit√© de l\'Air √† Nairobi',
        pays: 'üá∞üá™ Kenya',
        duree: '6-8 heures',
        difficulte: 'Interm√©diaire',
        couleur: 'green',
        icone: 'üåç',
        
        resume: 'Construisez un mod√®le ARMA de s√©ries temporelles pour pr√©dire les niveaux de particules fines (PM2.5) √† Nairobi. Apprenez √† extraire des donn√©es depuis MongoDB.',
        
        objectifs: [
            'Extraire des donn√©es depuis MongoDB avec pymongo',
            'Analyser et visualiser des s√©ries temporelles',
            'Tester la stationnarit√© avec le test ADF',
            'Construire un mod√®le ARMA/ARIMA',
            'Optimiser les hyperparam√®tres (p, q)'
        ],
        
        competences: {
            ingenierieDonnees: [
                {
                    nom: 'MongoDB et pymongo',
                    niveau: 'Interm√©diaire',
                    description: 'Connexion et requ√™tes NoSQL',
                    concepts: ['MongoClient', 'find()', 'projection', 'aggregation']
                },
                {
                    nom: 'Manipulation temporelle',
                    niveau: 'Interm√©diaire',
                    description: 'Index datetime et resampling',
                    concepts: ['DatetimeIndex', 'resample()', 'asfreq()', 'interpolate()']
                }
            ],
            seriesTemporelles: [
                {
                    nom: 'Stationnarit√©',
                    niveau: 'Interm√©diaire',
                    description: 'Test ADF et diff√©renciation',
                    concepts: ['adfuller', 'p-value', 'diff()', 'd parameter']
                },
                {
                    nom: 'ACF et PACF',
                    niveau: 'Interm√©diaire',
                    description: 'Visualiser l\'autocorr√©lation',
                    concepts: ['plot_acf', 'plot_pacf', 'lag', 'confidence interval']
                },
                {
                    nom: 'Mod√®le ARMA/ARIMA',
                    niveau: 'Avanc√©',
                    description: 'AutoRegressive Moving Average',
                    concepts: ['ARIMA', 'order=(p,d,q)', 'AR', 'MA', 'fit()', 'forecast()']
                }
            ]
        },
        
        modules: [
            {
                id: 1,
                titre: 'Extraction MongoDB',
                duree: '60 minutes',
                objectif: 'Connecter et extraire les donn√©es de pollution',
                contenu: [
                    {
                        type: 'code',
                        titre: 'Connexion et extraction',
                        langage: 'python',
                        code: `from pymongo import MongoClient
import pandas as pd

# Connexion
    client = MongoClient("mongodb://localhost:27017/")
    db = client["air_quality"]
    collection = db["nairobi_sensors"]

# Extraction
    cursor = collection.find(
        { "city": "Nairobi" },
        { "_id": 0, "timestamp": 1, "pm2_5": 1, "pm10": 1, "temperature": 1 }
    )

# Conversion en DataFrame
    df = pd.DataFrame(list(cursor))
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df.set_index('timestamp', inplace = True)
    df.sort_index(inplace = True)

    print(f"P√©riode: {df.index.min()} ‚Üí {df.index.max()}")
    df.head()`
                    }
                ]
            },
            {
                id: 2,
                titre: 'Analyse des s√©ries temporelles',
                duree: '90 minutes',
                objectif: 'Comprendre les patterns et tester la stationnarit√©',
                contenu: [
                    {
                        type: 'code',
                        titre: 'Test de stationnarit√© (ADF)',
                        langage: 'python',
                        code: `from statsmodels.tsa.stattools import adfuller

def test_stationarity(series, name = "Series"):
    result = adfuller(series.dropna())
    print(f"=== Test ADF: {name} ===")
    print(f"Statistique ADF: {result[0]:.4f}")
    print(f"P-value: {result[1]:.4f}")
    print(f"Stationnaire: {'OUI' if result[1] < 0.05 else 'NON'}")
    return result[1] < 0.05

# Test sur PM2.5
    is_stationary = test_stationarity(df['pm2_5'], "PM2.5")

# Si non stationnaire, diff√©rencier
    if not is_stationary:
        df['pm2_5_diff'] = df['pm2_5'].diff()
    test_stationarity(df['pm2_5_diff'].dropna(), "PM2.5 (diff√©renci√©e)")`
                    }
                ]
            },
            {
                id: 3,
                titre: 'Mod√©lisation ARMA',
                duree: '90 minutes',
                objectif: 'Construire et √©valuer le mod√®le',
                contenu: [
                    {
                        type: 'code',
                        titre: 'Mod√®le ARIMA',
                        langage: 'python',
                        code: `from statsmodels.tsa.arima.model import ARIMA
        from sklearn.metrics import mean_absolute_error

# Split temporel
    train_size = int(len(df) * 0.8)
    train, test = df['pm2_5'][:train_size], df['pm2_5'][train_size:]

# Mod√®le ARIMA(2, 1, 1)
    model = ARIMA(train, order = (2, 1, 1))
    results = model.fit()

# R√©sum√©
    print(results.summary())

# Pr√©visions
    forecast = results.forecast(steps = len(test))
    mae = mean_absolute_error(test, forecast)
    print(f"\\nMAE sur le test set: {mae:.2f} ¬µg/m¬≥")`
                    }
                ]
            },
            {
                id: 4,
                titre: 'Optimisation des hyperparam√®tres',
                duree: '60 minutes',
                objectif: 'Trouver les meilleurs param√®tres (p, d, q)',
                contenu: [
                    {
                        type: 'code',
                        titre: 'Grid Search sur (p, q)',
                        langage: 'python',
                        code: `import itertools
import warnings
warnings.filterwarnings('ignore')

# Grille de param√®tres
    p_values = range(0, 4)
    d_values = [0, 1]
    q_values = range(0, 4)

    best_aic = float('inf')
    best_order = None

    for p, d, q in itertools.product(p_values, d_values, q_values):
        try:
    model = ARIMA(train, order = (p, d, q))
    results = model.fit()
    if results.aic < best_aic:
        best_aic = results.aic
    best_order = (p, d, q)
    except:
    continue

    print(f"Meilleur ordre: {best_order}")
    print(f"AIC: {best_aic:.2f}")

# Mod√®le final
    final_model = ARIMA(train, order = best_order).fit()
    print(final_model.summary())`
                    }
                ]
            }
        ],
        
        dataset: {
            source: 'Capteurs de qualit√© d\'air Nairobi',
            taille: '2+ ann√©es de mesures horaires',
            colonnes: ['timestamp', 'pm2_5', 'pm10', 'temperature', 'humidity'],
            format: 'MongoDB'
        },
        
        outils: ['Python', 'pymongo', 'statsmodels', 'Pandas', 'Matplotlib'],
        
        prerequis: [
            'Projets 1-2 compl√©t√©s',
            'Notions de bases de donn√©es',
            'Bases des s√©ries temporelles'
        ]
    },

    // ==================================================================================
    // PROJET 4-8 : STRUCTURE SIMPLIFI√âE (m√™me format)
    // ==================================================================================
    {
        id: 'earthquake-nepal',
        numero: 4,
        titre: 'S√©isme au N√©pal',
        pays: 'üá≥üáµ N√©pal',
        duree: '8-10 heures',
        difficulte: 'Interm√©diaire',
        couleur: 'orange',
        icone: 'üèöÔ∏è',
        resume: 'Classification des d√©g√¢ts sismiques avec r√©gression logistique et arbres de d√©cision. Explorez les biais dans les donn√©es.',
        
        objectifs: [
            'Extraire des donn√©es depuis SQLite',
            'Construire des mod√®les de classification',
            'Analyser les biais discriminatoires',
            '√âvaluer l\'√©quit√© des mod√®les'
        ],
        
        competences: {
            ingenierieDonnees: [
                { nom: 'SQLite et SQL', niveau: 'Interm√©diaire', description: 'Requ√™tes SQL avec sqlite3', concepts: ['sqlite3.connect()', 'pd.read_sql()', 'JOIN', 'WHERE'] }
            ],
            machineLearning: [
                { nom: 'R√©gression logistique', niveau: 'Interm√©diaire', description: 'Classification binaire et multiclasse', concepts: ['LogisticRegression', 'multiclass', 'predict_proba()'] },
                { nom: 'Arbres de d√©cision', niveau: 'Interm√©diaire', description: 'Mod√®le interpr√©table', concepts: ['DecisionTreeClassifier', 'max_depth', 'feature_importances_'] }
            ],
            ethique: [
                { nom: 'D√©tection de biais', niveau: 'Avanc√©', description: 'Identifier les discriminations', concepts: ['demographic parity', 'equalized odds', 'fairness metrics'] }
            ]
        },
        
        modules: [
            { id: 1, titre: 'Extraction SQLite', duree: '60 minutes', objectif: 'Connecter et extraire les donn√©es' },
            { id: 2, titre: 'Pr√©paration des donn√©es', duree: '90 minutes', objectif: 'Encoder et √©quilibrer' },
            { id: 3, titre: 'Mod√®les de classification', duree: '120 minutes', objectif: 'Entra√Æner et comparer' },
            { id: 4, titre: 'Analyse des biais', duree: '60 minutes', objectif: '√âvaluer l\'√©quit√©' }
        ],
        
        dataset: { source: 'Nepal Earthquake Open Data', taille: '260 000+ b√¢timents', format: 'SQLite' },
        outils: ['Python', 'Scikit-learn', 'SQLite', 'Pandas'],
        prerequis: ['Projets 1-3 compl√©t√©s']
    },
    
    {
        id: 'bankruptcy-poland',
        numero: 5,
        titre: 'Faillite en Pologne',
        pays: 'üáµüá± Pologne',
        duree: '8-10 heures',
        difficulte: 'Avanc√©',
        couleur: 'red',
        icone: 'üìâ',
        resume: 'Pr√©disez les faillites d\'entreprises avec Random Forest et Gradient Boosting. G√©rez les donn√©es d√©s√©quilibr√©es.',
        
        objectifs: [
            'Ma√Ætriser la ligne de commande Linux',
            'G√©rer les classes d√©s√©quilibr√©es avec SMOTE',
            'Construire des mod√®les d\'ensemble',
            'Comprendre precision vs recall'
        ],
        
        competences: {
            ingenierieDonnees: [
                { nom: 'Linux CLI', niveau: 'Interm√©diaire', description: 'Navigation et manipulation', concepts: ['cd', 'ls', 'cat', 'head', 'grep', 'pipe'] },
                { nom: 'R√©√©chantillonnage', niveau: 'Avanc√©', description: 'SMOTE et undersampling', concepts: ['SMOTE', 'RandomUnderSampler', 'imblearn'] }
            ],
            machineLearning: [
                { nom: 'Random Forest', niveau: 'Avanc√©', description: 'Ensemble de d√©cision', concepts: ['RandomForestClassifier', 'n_estimators', 'feature_importances_'] },
                { nom: 'Gradient Boosting', niveau: 'Avanc√©', description: 'Boosting s√©quentiel', concepts: ['GradientBoostingClassifier', 'learning_rate', 'n_estimators'] }
            ],
            evaluation: [
                { nom: 'Precision/Recall', niveau: 'Avanc√©', description: 'M√©triques pour classes d√©s√©quilibr√©es', concepts: ['precision_score', 'recall_score', 'f1_score', 'trade-off'] },
                { nom: 'Courbe ROC', niveau: 'Avanc√©', description: '√âvaluation globale', concepts: ['roc_curve', 'roc_auc_score', 'seuil de d√©cision'] }
            ]
        },
        
        modules: [
            { id: 1, titre: 'Linux CLI', duree: '60 minutes', objectif: 'Naviguer en ligne de commande' },
            { id: 2, titre: 'Gestion du d√©s√©quilibre', duree: '90 minutes', objectif: 'Appliquer SMOTE' },
            { id: 3, titre: 'Mod√®les d\'ensemble', duree: '120 minutes', objectif: 'Entra√Æner et tuner' },
            { id: 4, titre: 'M√©triques m√©tier', duree: '60 minutes', objectif: 'Choisir le bon seuil' }
        ],
        
        dataset: { source: 'Polish Bankruptcy Dataset', taille: '10 000+ entreprises', format: 'CSV' },
        outils: ['Python', 'Scikit-learn', 'imbalanced-learn', 'Linux Bash'],
        prerequis: ['Projets 1-4 compl√©t√©s']
    },
    
    {
        id: 'customer-segmentation-us',
        numero: 6,
        titre: 'Segmentation Client aux USA',
        pays: 'üá∫üá∏ √âtats-Unis',
        duree: '8-10 heures',
        difficulte: 'Avanc√©',
        couleur: 'purple',
        icone: 'üë•',
        resume: 'Segmentez les consommateurs am√©ricains avec K-Means et visualisez avec PCA. Cr√©ez un dashboard interactif.',
        
        objectifs: [
            'Construire un mod√®le K-Means',
            'Appliquer PCA pour la visualisation',
            'Cr√©er un dashboard avec Plotly Dash',
            'Interpr√©ter les segments clients'
        ],
        
        competences: {
            apprentissageNonSupervise: [
                { nom: 'K-Means', niveau: 'Avanc√©', description: 'Clustering par centro√Ødes', concepts: ['KMeans', 'n_clusters', 'inertia_', 'labels_'] },
                { nom: 'M√©thode du coude', niveau: 'Interm√©diaire', description: 'Choisir K optimal', concepts: ['elbow method', 'inertia', 'silhouette_score'] }
            ],
            reductionDimensionnalite: [
                { nom: 'PCA', niveau: 'Avanc√©', description: 'Analyse en Composantes Principales', concepts: ['PCA', 'n_components', 'explained_variance_ratio_'] }
            ],
            developpementWeb: [
                { nom: 'Plotly Dash', niveau: 'Avanc√©', description: 'Dashboards interactifs Python', concepts: ['Dash', 'dcc', 'html', 'callback', 'Input', 'Output'] }
            ]
        },
        
        modules: [
            { id: 1, titre: 'Pr√©paration des donn√©es', duree: '60 minutes', objectif: 'Normaliser et explorer' },
            { id: 2, titre: 'Clustering K-Means', duree: '90 minutes', objectif: 'Segmenter les clients' },
            { id: 3, titre: 'PCA et Visualisation', duree: '90 minutes', objectif: 'R√©duire et visualiser' },
            { id: 4, titre: 'Dashboard Plotly Dash', duree: '120 minutes', objectif: 'Cr√©er l\'interface' }
        ],
        
        dataset: { source: 'US Consumer Survey', taille: '50 000+ consommateurs', format: 'CSV' },
        outils: ['Python', 'Scikit-learn', 'Plotly', 'Dash'],
        prerequis: ['Projets 1-5 compl√©t√©s']
    },
    
    {
        id: 'ab-testing-wqu',
        numero: 7,
        titre: 'Test A/B WorldQuant',
        pays: 'üåê Global',
        duree: '8-10 heures',
        difficulte: 'Avanc√©',
        couleur: 'indigo',
        icone: 'üìß',
        resume: 'Menez un test Chi-carr√© pour √©valuer l\'effet des emails sur les inscriptions. Cr√©ez une application three-tiered.',
        
        objectifs: [
            'Concevoir une exp√©rience A/B',
            'Effectuer un test Chi-carr√©',
            'Construire des classes ETL Python',
            'Impl√©menter une architecture three-tiered'
        ],
        
        competences: {
            statistiques: [
                { nom: 'Test Chi-carr√©', niveau: 'Avanc√©', description: 'Test d\'ind√©pendance', concepts: ['chi2_contingency', 'contingency table', 'p-value', 'alpha'] },
                { nom: 'Design d\'exp√©rience', niveau: 'Interm√©diaire', description: 'Groupes contr√¥le et test', concepts: ['randomization', 'sample size', 'power'] }
            ],
            geniieLogiciel: [
                { nom: 'Classes Python', niveau: 'Avanc√©', description: 'Programmation orient√©e objet', concepts: ['class', '__init__', 'methods', 'encapsulation'] },
                { nom: 'ETL Pipeline', niveau: 'Avanc√©', description: 'Extract Transform Load', concepts: ['Extractor', 'Transformer', 'Loader', 'orchestration'] },
                { nom: 'Architecture three-tiered', niveau: 'Avanc√©', description: 'S√©paration des couches', concepts: ['data layer', 'business layer', 'presentation layer'] }
            ]
        },
        
        modules: [
            { id: 1, titre: 'Design de l\'exp√©rience', duree: '60 minutes', objectif: 'D√©finir le test A/B' },
            { id: 2, titre: 'Classes ETL Python', duree: '120 minutes', objectif: 'Construire le pipeline' },
            { id: 3, titre: 'Analyse statistique', duree: '90 minutes', objectif: 'Test Chi-carr√©' },
            { id: 4, titre: 'Application Three-Tiered', duree: '90 minutes', objectif: 'Int√©grer les couches' }
        ],
        
        dataset: { source: 'WQU Enrollment Experiment', taille: '10 000+ prospects', format: 'SQLite' },
        outils: ['Python', 'SciPy', 'SQLite', 'Flask/Dash'],
        prerequis: ['Projets 1-6 compl√©t√©s']
    },
    
    {
        id: 'volatility-india',
        numero: 8,
        titre: 'Volatilit√© en Inde',
        pays: 'üáÆüá≥ Inde',
        duree: '10-12 heures',
        difficulte: 'Expert',
        couleur: 'amber',
        icone: 'üìà',
        resume: 'Cr√©ez un mod√®le GARCH pour pr√©dire la volatilit√© boursi√®re et construisez votre propre API REST.',
        
        objectifs: [
            'Acqu√©rir des donn√©es via API financi√®re',
            'Construire un mod√®le GARCH',
            'Stocker les donn√©es dans SQLite',
            'Cr√©er une API REST avec FastAPI'
        ],
        
        competences: {
            seriesTemporelles: [
                { nom: 'GARCH', niveau: 'Expert', description: 'Mod√©lisation de volatilit√©', concepts: ['arch_model', 'GARCH(1,1)', 'omega', 'alpha', 'beta'] },
                { nom: 'Rendements financiers', niveau: 'Avanc√©', description: 'Log returns et volatilit√©', concepts: ['log returns', 'rolling volatility', 'annualization'] }
            ],
            ingenierieDonnees: [
                { nom: 'API Finance', niveau: 'Avanc√©', description: 'Acquisition de donn√©es', concepts: ['yfinance', 'requests', 'rate limiting'] },
                { nom: 'Stockage SQLite', niveau: 'Interm√©diaire', description: 'Base de donn√©es locale', concepts: ['sqlite3', 'to_sql()', 'read_sql()'] }
            ],
            developpementAPI: [
                { nom: 'FastAPI', niveau: 'Avanc√©', description: 'Cr√©ation d\'endpoints REST', concepts: ['FastAPI', 'Pydantic', 'endpoints', 'uvicorn'] },
                { nom: 'Documentation API', niveau: 'Interm√©diaire', description: 'Swagger/OpenAPI', concepts: ['docs', 'schema', 'response_model'] }
            ]
        },
        
        modules: [
            { id: 1, titre: 'Acquisition via API', duree: '90 minutes', objectif: 'T√©l√©charger les donn√©es' },
            { id: 2, titre: 'Analyse de volatilit√©', duree: '90 minutes', objectif: 'Calculer et visualiser' },
            { id: 3, titre: 'Mod√©lisation GARCH', duree: '120 minutes', objectif: 'Entra√Æner et pr√©voir' },
            { id: 4, titre: 'API de pr√©diction', duree: '120 minutes', objectif: 'D√©ployer FastAPI' }
        ],
        
        dataset: { source: 'Yahoo Finance API (Nifty 50)', taille: '5+ ann√©es de donn√©es', format: 'API ‚Üí SQLite' },
        outils: ['Python', 'arch', 'FastAPI', 'yfinance', 'SQLite'],
        prerequis: ['Tous les projets pr√©c√©dents compl√©t√©s']
    }
];

// Fonction pour obtenir un projet par ID
export const getProjetById = (id) => {
    return dataScienceProjects.find(p => p.id === id);
};

// Statistiques globales
export const statsDataScience = {
    nombreProjets: 8,
    nombrePays: 8,
    dur√©eTotale: '60-80 heures',
    competencesCles: [
        'Python & Pandas',
        'Scikit-learn',
        'S√©ries temporelles',
        'Machine Learning',
        'SQL & NoSQL',
        'APIs & Dashboards'
    ],
    niveaux: {
        debutant: 1,
        intermediaire: 4,
        avance: 2,
        expert: 1
    }
};
